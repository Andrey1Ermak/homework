{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import torch\nfrom torch.optim import Adam\nfrom tqdm import tqdm\n\nimport pandas as pd\nfrom transformers import pipeline",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%%\n"
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "2023-03-02 22:42:21.791232: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n2023-03-02 22:42:23.200980: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n\n2023-03-02 22:42:23.201177: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n\n2023-03-02 22:42:23.201186: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "sentiment = pipeline(\"text-classification\", model='Tatyana/rubert-base-cased-sentiment-new')\nsentiment(\"Этот ресторан отличный\")",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9541073441505432}]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df_train = pd.read_csv(\"train.csv\")\ndf_val = pd.read_csv(\"val.csv\")\n\ndf_train.shape, df_val.shape",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((181467, 3), (22683, 3))"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df_train.head()",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                               text  class\n",
              "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
              "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
              "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
              "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
              "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df_train['class'].value_counts()",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    92063\n",
              "0    89404\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "idx = 11\nprint(df_train.iloc[idx]['text'])\nprint('label is', df_train.iloc[idx]['class'])\nsentiment(df_train.iloc[idx]['text'])",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "мартовские путёвки дорожают на глазах. только пару дней назад были за 66, уже 86 о_О\n\nlabel is 0\n"
        },
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9948994517326355}]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "df_train['text'] = df_train['text'].apply(lambda x: x.lower())\ndf_val['text'] = df_val['text'].apply(lambda x: x.lower())",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('Tatyana/rubert-base-cased-sentiment-new')\n\nexample_text = 'Пример текста для токенизации'\nbert_input = tokenizer(example_text, padding='max_length', max_length=9,\n                       truncation=True, return_tensors=\"pt\")\n\n\nprint(bert_input['input_ids'])\nprint(bert_input['attention_mask'])",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "tensor([[  101, 17863, 10316,   949, 15427,   831,  2703,   102,     0]])\n\ntensor([[1, 1, 1, 1, 1, 1, 1, 1, 0]])\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "tokenizer.ids_to_tokens[14337], tokenizer.ids_to_tokens[77572]",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('пирами', 'уедут')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "example_text = tokenizer.decode(bert_input.input_ids[0])\n\nprint(example_text)",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[CLS] Пример текста для токенизации [SEP] [PAD]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "class TwitterDataset(torch.utils.data.Dataset):\n\n    def __init__(self, txts, labels):\n        self._labels = labels\n\n        self.tokenizer = BertTokenizer.from_pretrained('Tatyana/rubert-base-cased-sentiment-new')\n        self._txts = [self.tokenizer(text, padding='max_length', max_length=9,\n                                     truncation=True, return_tensors=\"pt\")\n                      for text in txts]\n\n    def __len__(self):\n        return len(self._txts)\n\n    def __getitem__(self, index):\n        return self._txts[index], self._labels[index]",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y_train = df_train['class'].values\ny_val = df_val['class'].values\n\ntrain_dataset = TwitterDataset(df_train['text'], y_train)\nvalid_dataset = TwitterDataset(df_val['text'], y_val)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                          batch_size=64,\n                          shuffle=True,\n                          num_workers=2)\nvalid_loader = torch.utils.data.DataLoader(valid_dataset,\n                          batch_size=64,\n                          shuffle=False,\n                          num_workers=1)",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "for txt, lbl in train_loader:\n    print(txt.keys())\n    print(txt['input_ids'].shape)\n    break",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\ndict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n\ntorch.Size([64, 1, 9])\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "from torch import nn\nfrom transformers import BertModel\n\n\nclass BertClassifier(nn.Module):\n\n    def __init__(self, dropout=0.5):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('Tatyana/rubert-base-cased-sentiment-new')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 2)\n        self.sigm = nn.Sigmoid()\n\n    def forward(self, x):\n\n        _, pooled_output = self.bert(input_ids=x, return_dict=False)\n        # _, pooled_output - набор эмбеддинигов слов, эмбеддинг предложения\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        final_layer = self.sigm(linear_output)\n        return final_layer",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "execution_count": 15,
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "model = BertClassifier().to(device)\ncriterion = nn.CrossEntropyLoss()\n\n# optimizer = Adam(model.parameters(), lr=0.001)  # полное обучение\noptimizer = Adam(model.linear.parameters(), lr=0.001)  # неполное обучение",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Some weights of the model checkpoint at Tatyana/rubert-base-cased-sentiment-new were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "print(model)\nprint(\"Parameters full train:\", sum([param.nelement() for param in model.parameters()]))\nprint(\"Parameters transfer learning:\", sum([param.nelement() for param in model.linear.parameters()]))",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "BertClassifier(\n\n  (bert): BertModel(\n\n    (embeddings): BertEmbeddings(\n\n      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n\n      (position_embeddings): Embedding(512, 768)\n\n      (token_type_embeddings): Embedding(2, 768)\n\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n      (dropout): Dropout(p=0.1, inplace=False)\n\n    )\n\n    (encoder): BertEncoder(\n\n      (layer): ModuleList(\n\n        (0): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (1): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (2): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (3): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (4): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (5): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (6): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (7): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (8): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (9): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (10): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n        (11): BertLayer(\n\n          (attention): BertAttention(\n\n            (self): BertSelfAttention(\n\n              (query): Linear(in_features=768, out_features=768, bias=True)\n\n              (key): Linear(in_features=768, out_features=768, bias=True)\n\n              (value): Linear(in_features=768, out_features=768, bias=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n            (output): BertSelfOutput(\n\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n              (dropout): Dropout(p=0.1, inplace=False)\n\n            )\n\n          )\n\n          (intermediate): BertIntermediate(\n\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n\n            (intermediate_act_fn): GELUActivation()\n\n          )\n\n          (output): BertOutput(\n\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n\n            (dropout): Dropout(p=0.1, inplace=False)\n\n          )\n\n        )\n\n      )\n\n    )\n\n    (pooler): BertPooler(\n\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n\n      (activation): Tanh()\n\n    )\n\n  )\n\n  (dropout): Dropout(p=0.5, inplace=False)\n\n  (linear): Linear(in_features=768, out_features=2, bias=True)\n\n  (sigm): Sigmoid()\n\n)\n\nParameters full train: 177854978\n\nParameters transfer learning: 1538\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "model.eval()\ntotal_loss_val_before, total_acc_val_before = 0.0, 0.0\nfor val_input, val_label in valid_loader:\n    val_label = val_label.to(device)\n    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n    output = model(input_id)\n\n    batch_loss = criterion(output, val_label)\n    total_loss_val_before += batch_loss.item()\n\n    acc = (output.argmax(dim=1) == val_label).sum().item()\n    total_acc_val_before += acc\nprint(\n    f'Val Loss before: {total_loss_val_before / len(valid_dataset): .3f} \\\n    | Val Accuracy before: {total_acc_val_before / len(valid_dataset): .3f}'\n)",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\nVal Loss before:  0.011     | Val Accuracy before:  0.427\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "total_acc_train = 0\ntotal_loss_train = 0\nfor epoch_num in range(2):\n\n    model.train()\n    for train_input, train_label in tqdm(train_loader):\n        input_id = train_input['input_ids'].squeeze(1).to(device)\n        train_label = train_label.to(device)\n\n        output = model(input_id)\n\n        batch_loss = criterion(output, train_label)\n        total_loss_train += batch_loss.item()\n\n        acc = (output.argmax(dim=1) == train_label).sum().item()\n        total_acc_train += acc\n\n        model.zero_grad()\n        batch_loss.backward()\n        optimizer.step()\nprint(\n    f'Train Loss: {total_loss_train / len(train_dataset): .3f} \\\n    | Train Accuracy: {total_acc_train / len(train_dataset): .3f}'\n)",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "  0%|          | 0/2836 [00:00<?, ?it/s]"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "100%|██████████| 2836/2836 [1:52:43<00:00,  2.38s/it]\n\n  0%|          | 0/2836 [00:00<?, ?it/s]"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "100%|██████████| 2836/2836 [1:51:11<00:00,  2.35s/it]"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Train Loss:  0.021     | Train Accuracy:  1.152\n"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "model.eval()\ntotal_loss_val_after, total_acc_val_after = 0.0, 0.0\nfor val_input, val_label in valid_loader:\n    val_label = val_label.to(device)\n    input_id = val_input['input_ids'].squeeze(1).to(device)\n\n    output = model(input_id)\n\n    batch_loss = criterion(output, val_label)\n    total_loss_val_after += batch_loss.item()\n\n    acc = (output.argmax(dim=1) == val_label).sum().item()\n    total_acc_val_after += acc\nprint(\n    f'Val Loss after: {total_loss_val_after / len(valid_dataset): .3f} \\\n    | Val Accuracy after: {total_acc_val_after / len(valid_dataset): .3f}'\n)",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n\nTo disable this warning, you can either:\n\n\t- Avoid using `tokenizers` before the fork if possible\n\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\nVal Loss after:  0.011     | Val Accuracy after:  0.574\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "print(\n    f'Val Loss before: {total_loss_val_before / len(valid_dataset): .3f} \\\n    | Val Accuracy before: {total_acc_val_before / len(valid_dataset): .3f} \\\n    | Train Loss: {total_loss_train / len(train_dataset): .3f} \\\n    | Train Accuracy: {total_acc_train / len(train_dataset): .3f} \\\n    | Val Loss after: {total_loss_val_after / len(valid_dataset): .3f} \\\n    | Val Accuracy after: {total_acc_val_after / len(valid_dataset): .3f}'\n)",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Val Loss before:  0.011     | Val Accuracy before:  0.427     | Train Loss:  0.021     | Train Accuracy:  1.152     | Val Loss after:  0.011     | Val Accuracy after:  0.574\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}