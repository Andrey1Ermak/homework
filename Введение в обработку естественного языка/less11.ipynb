{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# Урок 11. Модель Transformer-1\n",
    "\n",
    "Разобраться с моделью перевода (с механизмом внимания) как она устроена, запустить для перевода с русского на английский (при желании можно взять другие пары языков)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNvjhDyAKk3U",
    "outputId": "f1175007-915c-4e91-dbfd-195b103c12da"
   },
   "outputs": [],
   "source": [
    "# !wget http://www.manythings.org/anki/rus-eng.zip\n",
    "# !mkdir rus-eng\n",
    "# !unzip rus-eng.zip -d rus-eng/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eAY9k49G3jE_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = w.lower().strip()\n",
    "\n",
    "  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "  w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,']+\", \" \", w)\n",
    "\n",
    "  w = w.strip()\n",
    "\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w\n",
    "\n",
    "def create_dataset(path, num_examples):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]]  for l in lines[:num_examples]]\n",
    "\n",
    "  return zip(*word_pairs)\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kRVATYOgJs1b"
   },
   "outputs": [],
   "source": [
    "path_to_file = \"rus-eng/rus.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42
    },
    "id": "yV9lZXQXNbnH",
    "outputId": "4d24345f-2b72-4677-ccf9-ce8d565dbb97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> i can't go . <end>\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(\"I can't go.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cTbSbBz55QtF",
    "outputId": "510338b5-1a6c-451f-ecff-0631321fd7be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
      "<start> несомненно , для каждого мужчины в этом мире где то есть подходящая женщина , которая может стать ему женой , обратное верно и для женщин . но если учесть , что у человека может быть максимум несколько сотен знакомых , из которых лишь дюжина , а то и меньше , тех , кого он знает близко , а из этой дюжины у него один или от силы два друга , то можно легко увидеть , что с уч том миллионов живущих на земле людей , ни один подходящий мужчина , возможно , ещ не встретил подходящую женщину . <end>\n"
     ]
    }
   ],
   "source": [
    "en, ru = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(ru[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8j9g9AnIeZV",
    "outputId": "ad408740-7b5a-4009-b142-43c77b84d81f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479223, 479223)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "cnxC7q-j3jFD"
   },
   "outputs": [],
   "source": [
    "num_examples = 200000 # Ограничиваем размер датасета для ускорения обучения\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QILQkOs3jFG",
    "outputId": "c2a63b29-c70f-4e19-ba1e-6a66e05c27b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000 160000 40000 40000\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXukARTDd7MT",
    "outputId": "9e1bc814-2fb6-4534-b8ad-e326e1e9bcce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "15 ----> у\n",
      "17 ----> меня\n",
      "351 ----> хорошая\n",
      "2918 ----> квартира\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> i\n",
      "22 ----> have\n",
      "9 ----> a\n",
      "381 ----> nice\n",
      "1186 ----> apartment\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TqHsArVZ3jFS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 14:48:12.741864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15406 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:a1:00.0, compute capability: 6.0\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qc6-NK1GtWQt",
    "outputId": "8399ed1f-ac05-4b72-f618-dd2ff0b474ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 12]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "nZ2rI24i3jFg"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60gSVh05Jl6l",
    "outputId": "9843bcf8-7fc2-4f68-d7c8-fad8c95ca9cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 14:48:13.295918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8905\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "#инициализируем начальное скрытое состояние из нулей\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "#получаем выход энкодера и последнее скрытое состояние\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "YTyhL28Niqk1"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "\n",
    "    # применяем к векторам скрытого состояния и выходов энкодера полносвязный слой (выход (batch_size, 1, units) и (batch_size, max_length, units))\n",
    "    # складываем полученные векторы, применяем к сумму тангенс (batch_size, max_length, units)\n",
    "    # проводим результат через dense слой (batch_size, max_length, 1)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    # получаем вероятностное распределение\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    # умножаем веса внимания на векторы значенй выход (batch_size, max_len, hidden size)\n",
    "    context_vector = attention_weights * values\n",
    "    # находим вдоль столбцов (batch_size, hidden_size)\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUqssWcci1XJ",
    "outputId": "c0504f79-076d-404e-9168-ef26fb1ec1a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "#создаем слой внимания\n",
    "attention_layer = BahdanauAttention(10)\n",
    "#передаем выход энкодера и его скрытое состояние\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhCQue5JtatY"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yJ_B3mhW3jFk"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # используем слой внимания\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    # получаем выходы слоя внимания (из скрытого состояния и выхода энкодера)\n",
    "    # context_vector shape == (batch_size, hidden_size)\n",
    "    # attention_weights shape == (batch_size, max_len, 1)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    #соединяем выход эмбеддинга с вектором контекста и подаем навход RNN\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5UY8wko3jFp",
    "outputId": "16189951-c048-4ce7-d8f9-7260f9d8db41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 10316)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "#применяем декодер к случайному батчу из равномерного распределения (батч,1) и выходам энкодера\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "WmTHr5iV3jFr"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Zj8bXQTgNwrF"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_attention_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "sC9ArXSsVfqn"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    #получаем выходы encoder\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    #помещаем выходное скрытое состояние энкодера в скрытое состояние decoder\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    #формируем вход декодера:\n",
    "    # берем список длины батч из индексов тега \n",
    "    # приписываем списку размерность 1\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    #Teacher forcing - выводим target в качестве следующего входа\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      #помещаем enc_output, dec_input, dec_hidden в decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      # считаем функцию потерь\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # используем teacher forcing (приписываем списку размерность 1)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  #вычисляем градиенты loss по variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  #оптимизатор применяет подсчитанные градиенты\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddefjBMa3jF0",
    "outputId": "0dbc3443-6d93-45fb-e15e-d50461f65b81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-04 14:48:23.237945: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f0314145d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-04 14:48:23.237996: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2023-11-04 14:48:23.251284: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-04 14:48:23.430759: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.9939\n",
      "Epoch 1 Batch 100 Loss 2.1782\n",
      "Epoch 1 Batch 200 Loss 2.1514\n",
      "Epoch 1 Batch 300 Loss 1.8102\n",
      "Epoch 1 Batch 400 Loss 1.7834\n",
      "Epoch 1 Batch 500 Loss 1.7076\n",
      "Epoch 1 Batch 600 Loss 1.5803\n",
      "Epoch 1 Batch 700 Loss 1.5541\n",
      "Epoch 1 Batch 800 Loss 1.4349\n",
      "Epoch 1 Batch 900 Loss 1.3519\n",
      "Epoch 1 Batch 1000 Loss 1.3739\n",
      "Epoch 1 Batch 1100 Loss 1.1072\n",
      "Epoch 1 Batch 1200 Loss 1.2607\n",
      "Epoch 1 Batch 1300 Loss 1.1246\n",
      "Epoch 1 Batch 1400 Loss 0.9899\n",
      "Epoch 1 Batch 1500 Loss 1.0838\n",
      "Epoch 1 Batch 1600 Loss 0.9103\n",
      "Epoch 1 Batch 1700 Loss 0.9883\n",
      "Epoch 1 Batch 1800 Loss 0.9719\n",
      "Epoch 1 Batch 1900 Loss 0.8219\n",
      "Epoch 1 Batch 2000 Loss 0.7865\n",
      "Epoch 1 Batch 2100 Loss 0.7370\n",
      "Epoch 1 Batch 2200 Loss 0.7156\n",
      "Epoch 1 Batch 2300 Loss 0.6850\n",
      "Epoch 1 Batch 2400 Loss 0.6432\n",
      "Epoch 1 Loss 1.2950\n",
      "Time taken for 1 epoch 181.63008856773376 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.6711\n",
      "Epoch 2 Batch 100 Loss 0.5861\n",
      "Epoch 2 Batch 200 Loss 0.6307\n",
      "Epoch 2 Batch 300 Loss 0.5366\n",
      "Epoch 2 Batch 400 Loss 0.5464\n",
      "Epoch 2 Batch 500 Loss 0.4396\n",
      "Epoch 2 Batch 600 Loss 0.5322\n",
      "Epoch 2 Batch 700 Loss 0.4928\n",
      "Epoch 2 Batch 800 Loss 0.5561\n",
      "Epoch 2 Batch 900 Loss 0.5588\n",
      "Epoch 2 Batch 1000 Loss 0.5260\n",
      "Epoch 2 Batch 1100 Loss 0.5310\n",
      "Epoch 2 Batch 1200 Loss 0.4230\n",
      "Epoch 2 Batch 1300 Loss 0.4861\n",
      "Epoch 2 Batch 1400 Loss 0.4102\n",
      "Epoch 2 Batch 1500 Loss 0.4965\n",
      "Epoch 2 Batch 1600 Loss 0.4535\n",
      "Epoch 2 Batch 1700 Loss 0.4492\n",
      "Epoch 2 Batch 1800 Loss 0.4216\n",
      "Epoch 2 Batch 1900 Loss 0.3982\n",
      "Epoch 2 Batch 2000 Loss 0.4336\n",
      "Epoch 2 Batch 2100 Loss 0.3774\n",
      "Epoch 2 Batch 2200 Loss 0.5092\n",
      "Epoch 2 Batch 2300 Loss 0.4615\n",
      "Epoch 2 Batch 2400 Loss 0.4101\n",
      "Epoch 2 Loss 0.4992\n",
      "Time taken for 1 epoch 158.09530472755432 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.3025\n",
      "Epoch 3 Batch 100 Loss 0.3013\n",
      "Epoch 3 Batch 200 Loss 0.2558\n",
      "Epoch 3 Batch 300 Loss 0.3371\n",
      "Epoch 3 Batch 400 Loss 0.2510\n",
      "Epoch 3 Batch 500 Loss 0.2718\n",
      "Epoch 3 Batch 600 Loss 0.3042\n",
      "Epoch 3 Batch 700 Loss 0.2289\n",
      "Epoch 3 Batch 800 Loss 0.2540\n",
      "Epoch 3 Batch 900 Loss 0.3564\n",
      "Epoch 3 Batch 1000 Loss 0.2204\n",
      "Epoch 3 Batch 1100 Loss 0.3480\n",
      "Epoch 3 Batch 1200 Loss 0.3260\n",
      "Epoch 3 Batch 1300 Loss 0.2865\n",
      "Epoch 3 Batch 1400 Loss 0.2934\n",
      "Epoch 3 Batch 1500 Loss 0.3118\n",
      "Epoch 3 Batch 1600 Loss 0.3167\n",
      "Epoch 3 Batch 1700 Loss 0.3442\n",
      "Epoch 3 Batch 1800 Loss 0.2112\n",
      "Epoch 3 Batch 1900 Loss 0.2530\n",
      "Epoch 3 Batch 2000 Loss 0.2830\n",
      "Epoch 3 Batch 2100 Loss 0.2573\n",
      "Epoch 3 Batch 2200 Loss 0.2869\n",
      "Epoch 3 Batch 2300 Loss 0.3793\n",
      "Epoch 3 Batch 2400 Loss 0.2288\n",
      "Epoch 3 Loss 0.2904\n",
      "Time taken for 1 epoch 158.0284378528595 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1910\n",
      "Epoch 4 Batch 100 Loss 0.1616\n",
      "Epoch 4 Batch 200 Loss 0.1266\n",
      "Epoch 4 Batch 300 Loss 0.1756\n",
      "Epoch 4 Batch 400 Loss 0.1803\n",
      "Epoch 4 Batch 500 Loss 0.1893\n",
      "Epoch 4 Batch 600 Loss 0.1647\n",
      "Epoch 4 Batch 700 Loss 0.1703\n",
      "Epoch 4 Batch 800 Loss 0.1435\n",
      "Epoch 4 Batch 900 Loss 0.1832\n",
      "Epoch 4 Batch 1000 Loss 0.2728\n",
      "Epoch 4 Batch 1100 Loss 0.1600\n",
      "Epoch 4 Batch 1200 Loss 0.2249\n",
      "Epoch 4 Batch 1300 Loss 0.2233\n",
      "Epoch 4 Batch 1400 Loss 0.2209\n",
      "Epoch 4 Batch 1500 Loss 0.1578\n",
      "Epoch 4 Batch 1600 Loss 0.1617\n",
      "Epoch 4 Batch 1700 Loss 0.2530\n",
      "Epoch 4 Batch 1800 Loss 0.1632\n",
      "Epoch 4 Batch 1900 Loss 0.2186\n",
      "Epoch 4 Batch 2000 Loss 0.2707\n",
      "Epoch 4 Batch 2100 Loss 0.2090\n",
      "Epoch 4 Batch 2200 Loss 0.1628\n",
      "Epoch 4 Batch 2300 Loss 0.1632\n",
      "Epoch 4 Batch 2400 Loss 0.2204\n",
      "Epoch 4 Loss 0.1986\n",
      "Time taken for 1 epoch 157.6494278907776 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.1183\n",
      "Epoch 5 Batch 100 Loss 0.1156\n",
      "Epoch 5 Batch 200 Loss 0.1155\n",
      "Epoch 5 Batch 300 Loss 0.1139\n",
      "Epoch 5 Batch 400 Loss 0.1883\n",
      "Epoch 5 Batch 500 Loss 0.1269\n",
      "Epoch 5 Batch 600 Loss 0.1742\n",
      "Epoch 5 Batch 700 Loss 0.1414\n",
      "Epoch 5 Batch 800 Loss 0.1552\n",
      "Epoch 5 Batch 900 Loss 0.1681\n",
      "Epoch 5 Batch 1000 Loss 0.1322\n",
      "Epoch 5 Batch 1100 Loss 0.1524\n",
      "Epoch 5 Batch 1200 Loss 0.2032\n",
      "Epoch 5 Batch 1300 Loss 0.1478\n",
      "Epoch 5 Batch 1400 Loss 0.1281\n",
      "Epoch 5 Batch 1500 Loss 0.1409\n",
      "Epoch 5 Batch 1600 Loss 0.1893\n",
      "Epoch 5 Batch 1700 Loss 0.2097\n",
      "Epoch 5 Batch 1800 Loss 0.1590\n",
      "Epoch 5 Batch 1900 Loss 0.1465\n",
      "Epoch 5 Batch 2000 Loss 0.1708\n",
      "Epoch 5 Batch 2100 Loss 0.1872\n",
      "Epoch 5 Batch 2200 Loss 0.1852\n",
      "Epoch 5 Batch 2300 Loss 0.1860\n",
      "Epoch 5 Batch 2400 Loss 0.1460\n",
      "Epoch 5 Loss 0.1509\n",
      "Time taken for 1 epoch 157.98704051971436 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0934\n",
      "Epoch 6 Batch 100 Loss 0.0907\n",
      "Epoch 6 Batch 200 Loss 0.1007\n",
      "Epoch 6 Batch 300 Loss 0.0825\n",
      "Epoch 6 Batch 400 Loss 0.1018\n",
      "Epoch 6 Batch 500 Loss 0.1217\n",
      "Epoch 6 Batch 600 Loss 0.1082\n",
      "Epoch 6 Batch 700 Loss 0.1019\n",
      "Epoch 6 Batch 800 Loss 0.1283\n",
      "Epoch 6 Batch 900 Loss 0.1090\n",
      "Epoch 6 Batch 1000 Loss 0.0965\n",
      "Epoch 6 Batch 1100 Loss 0.1295\n",
      "Epoch 6 Batch 1200 Loss 0.0897\n",
      "Epoch 6 Batch 1300 Loss 0.1184\n",
      "Epoch 6 Batch 1400 Loss 0.1266\n",
      "Epoch 6 Batch 1500 Loss 0.1193\n",
      "Epoch 6 Batch 1600 Loss 0.1391\n",
      "Epoch 6 Batch 1700 Loss 0.1674\n",
      "Epoch 6 Batch 1800 Loss 0.1751\n",
      "Epoch 6 Batch 1900 Loss 0.1188\n",
      "Epoch 6 Batch 2000 Loss 0.1029\n",
      "Epoch 6 Batch 2100 Loss 0.2071\n",
      "Epoch 6 Batch 2200 Loss 0.1355\n",
      "Epoch 6 Batch 2300 Loss 0.1272\n",
      "Epoch 6 Batch 2400 Loss 0.1596\n",
      "Epoch 6 Loss 0.1235\n",
      "Time taken for 1 epoch 157.7943148612976 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0765\n",
      "Epoch 7 Batch 100 Loss 0.0569\n",
      "Epoch 7 Batch 200 Loss 0.0632\n",
      "Epoch 7 Batch 300 Loss 0.1200\n",
      "Epoch 7 Batch 400 Loss 0.0859\n",
      "Epoch 7 Batch 500 Loss 0.0868\n",
      "Epoch 7 Batch 600 Loss 0.1037\n",
      "Epoch 7 Batch 700 Loss 0.0789\n",
      "Epoch 7 Batch 800 Loss 0.0604\n",
      "Epoch 7 Batch 900 Loss 0.1849\n",
      "Epoch 7 Batch 1000 Loss 0.1127\n",
      "Epoch 7 Batch 1100 Loss 0.1305\n",
      "Epoch 7 Batch 1200 Loss 0.0588\n",
      "Epoch 7 Batch 1300 Loss 0.1104\n",
      "Epoch 7 Batch 1400 Loss 0.0990\n",
      "Epoch 7 Batch 1500 Loss 0.0966\n",
      "Epoch 7 Batch 1600 Loss 0.1028\n",
      "Epoch 7 Batch 1700 Loss 0.0948\n",
      "Epoch 7 Batch 1800 Loss 0.1095\n",
      "Epoch 7 Batch 1900 Loss 0.1164\n",
      "Epoch 7 Batch 2000 Loss 0.0930\n",
      "Epoch 7 Batch 2100 Loss 0.1250\n",
      "Epoch 7 Batch 2200 Loss 0.1257\n",
      "Epoch 7 Batch 2300 Loss 0.1465\n",
      "Epoch 7 Batch 2400 Loss 0.1062\n",
      "Epoch 7 Loss 0.1062\n",
      "Time taken for 1 epoch 158.1117389202118 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0540\n",
      "Epoch 8 Batch 100 Loss 0.0653\n",
      "Epoch 8 Batch 200 Loss 0.0617\n",
      "Epoch 8 Batch 300 Loss 0.0655\n",
      "Epoch 8 Batch 400 Loss 0.0653\n",
      "Epoch 8 Batch 500 Loss 0.0634\n",
      "Epoch 8 Batch 600 Loss 0.1235\n",
      "Epoch 8 Batch 700 Loss 0.0863\n",
      "Epoch 8 Batch 800 Loss 0.1217\n",
      "Epoch 8 Batch 900 Loss 0.1347\n",
      "Epoch 8 Batch 1000 Loss 0.1003\n",
      "Epoch 8 Batch 1100 Loss 0.0911\n",
      "Epoch 8 Batch 1200 Loss 0.0633\n",
      "Epoch 8 Batch 1300 Loss 0.1022\n",
      "Epoch 8 Batch 1400 Loss 0.1032\n",
      "Epoch 8 Batch 1500 Loss 0.1187\n",
      "Epoch 8 Batch 1600 Loss 0.0970\n",
      "Epoch 8 Batch 1700 Loss 0.1269\n",
      "Epoch 8 Batch 1800 Loss 0.0856\n",
      "Epoch 8 Batch 1900 Loss 0.0893\n",
      "Epoch 8 Batch 2000 Loss 0.0794\n",
      "Epoch 8 Batch 2100 Loss 0.1004\n",
      "Epoch 8 Batch 2200 Loss 0.0669\n",
      "Epoch 8 Batch 2300 Loss 0.0872\n",
      "Epoch 8 Batch 2400 Loss 0.1086\n",
      "Epoch 8 Loss 0.0953\n",
      "Time taken for 1 epoch 157.88552904129028 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0799\n",
      "Epoch 9 Batch 100 Loss 0.0581\n",
      "Epoch 9 Batch 200 Loss 0.0487\n",
      "Epoch 9 Batch 300 Loss 0.1062\n",
      "Epoch 9 Batch 400 Loss 0.0683\n",
      "Epoch 9 Batch 500 Loss 0.0678\n",
      "Epoch 9 Batch 600 Loss 0.0656\n",
      "Epoch 9 Batch 700 Loss 0.0624\n",
      "Epoch 9 Batch 800 Loss 0.0559\n",
      "Epoch 9 Batch 900 Loss 0.0576\n",
      "Epoch 9 Batch 1000 Loss 0.1003\n",
      "Epoch 9 Batch 1100 Loss 0.0849\n",
      "Epoch 9 Batch 1200 Loss 0.0726\n",
      "Epoch 9 Batch 1300 Loss 0.0745\n",
      "Epoch 9 Batch 1400 Loss 0.1033\n",
      "Epoch 9 Batch 1500 Loss 0.1050\n",
      "Epoch 9 Batch 1600 Loss 0.1126\n",
      "Epoch 9 Batch 1700 Loss 0.1033\n",
      "Epoch 9 Batch 1800 Loss 0.1064\n",
      "Epoch 9 Batch 1900 Loss 0.1099\n",
      "Epoch 9 Batch 2000 Loss 0.0836\n",
      "Epoch 9 Batch 2100 Loss 0.1168\n",
      "Epoch 9 Batch 2200 Loss 0.0928\n",
      "Epoch 9 Batch 2300 Loss 0.0888\n",
      "Epoch 9 Batch 2400 Loss 0.1236\n",
      "Epoch 9 Loss 0.0869\n",
      "Time taken for 1 epoch 157.71977162361145 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0618\n",
      "Epoch 10 Batch 100 Loss 0.0807\n",
      "Epoch 10 Batch 200 Loss 0.0566\n",
      "Epoch 10 Batch 300 Loss 0.0564\n",
      "Epoch 10 Batch 400 Loss 0.0879\n",
      "Epoch 10 Batch 500 Loss 0.0750\n",
      "Epoch 10 Batch 600 Loss 0.0575\n",
      "Epoch 10 Batch 700 Loss 0.0913\n",
      "Epoch 10 Batch 800 Loss 0.0684\n",
      "Epoch 10 Batch 900 Loss 0.0689\n",
      "Epoch 10 Batch 1000 Loss 0.0773\n",
      "Epoch 10 Batch 1100 Loss 0.0826\n",
      "Epoch 10 Batch 1200 Loss 0.0790\n",
      "Epoch 10 Batch 1300 Loss 0.0947\n",
      "Epoch 10 Batch 1400 Loss 0.0869\n",
      "Epoch 10 Batch 1500 Loss 0.0871\n",
      "Epoch 10 Batch 1600 Loss 0.0940\n",
      "Epoch 10 Batch 1700 Loss 0.0713\n",
      "Epoch 10 Batch 1800 Loss 0.1149\n",
      "Epoch 10 Batch 1900 Loss 0.1094\n",
      "Epoch 10 Batch 2000 Loss 0.0962\n",
      "Epoch 10 Batch 2100 Loss 0.0960\n",
      "Epoch 10 Batch 2200 Loss 0.1085\n",
      "Epoch 10 Batch 2300 Loss 0.0889\n",
      "Epoch 10 Batch 2400 Loss 0.0650\n",
      "Epoch 10 Loss 0.0806\n",
      "Time taken for 1 epoch 157.9364116191864 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0650\n",
      "Epoch 11 Batch 100 Loss 0.0465\n",
      "Epoch 11 Batch 200 Loss 0.0684\n",
      "Epoch 11 Batch 300 Loss 0.0652\n",
      "Epoch 11 Batch 400 Loss 0.0804\n",
      "Epoch 11 Batch 500 Loss 0.1140\n",
      "Epoch 11 Batch 600 Loss 0.0517\n",
      "Epoch 11 Batch 700 Loss 0.0480\n",
      "Epoch 11 Batch 800 Loss 0.0867\n",
      "Epoch 11 Batch 900 Loss 0.0505\n",
      "Epoch 11 Batch 1000 Loss 0.0558\n",
      "Epoch 11 Batch 1100 Loss 0.0660\n",
      "Epoch 11 Batch 1200 Loss 0.0666\n",
      "Epoch 11 Batch 1300 Loss 0.0617\n",
      "Epoch 11 Batch 1400 Loss 0.0726\n",
      "Epoch 11 Batch 1500 Loss 0.1090\n",
      "Epoch 11 Batch 1600 Loss 0.0693\n",
      "Epoch 11 Batch 1700 Loss 0.0549\n",
      "Epoch 11 Batch 1800 Loss 0.0643\n",
      "Epoch 11 Batch 1900 Loss 0.0727\n",
      "Epoch 11 Batch 2000 Loss 0.0589\n",
      "Epoch 11 Batch 2100 Loss 0.0868\n",
      "Epoch 11 Batch 2200 Loss 0.1029\n",
      "Epoch 11 Batch 2300 Loss 0.0532\n",
      "Epoch 11 Batch 2400 Loss 0.0610\n",
      "Epoch 11 Loss 0.0763\n",
      "Time taken for 1 epoch 157.5768370628357 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0657\n",
      "Epoch 12 Batch 100 Loss 0.0867\n",
      "Epoch 12 Batch 200 Loss 0.0422\n",
      "Epoch 12 Batch 300 Loss 0.0479\n",
      "Epoch 12 Batch 400 Loss 0.0794\n",
      "Epoch 12 Batch 500 Loss 0.0808\n",
      "Epoch 12 Batch 600 Loss 0.0647\n",
      "Epoch 12 Batch 700 Loss 0.0373\n",
      "Epoch 12 Batch 800 Loss 0.0756\n",
      "Epoch 12 Batch 900 Loss 0.0672\n",
      "Epoch 12 Batch 1000 Loss 0.0544\n",
      "Epoch 12 Batch 1100 Loss 0.0639\n",
      "Epoch 12 Batch 1200 Loss 0.0606\n",
      "Epoch 12 Batch 1300 Loss 0.0853\n",
      "Epoch 12 Batch 1400 Loss 0.0694\n",
      "Epoch 12 Batch 1500 Loss 0.0663\n",
      "Epoch 12 Batch 1600 Loss 0.0909\n",
      "Epoch 12 Batch 1700 Loss 0.0830\n",
      "Epoch 12 Batch 1800 Loss 0.0821\n",
      "Epoch 12 Batch 1900 Loss 0.1012\n",
      "Epoch 12 Batch 2000 Loss 0.0891\n",
      "Epoch 12 Batch 2100 Loss 0.0762\n",
      "Epoch 12 Batch 2200 Loss 0.0834\n",
      "Epoch 12 Batch 2300 Loss 0.0719\n",
      "Epoch 12 Batch 2400 Loss 0.1046\n",
      "Epoch 12 Loss 0.0729\n",
      "Time taken for 1 epoch 157.74533200263977 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0568\n",
      "Epoch 13 Batch 100 Loss 0.0662\n",
      "Epoch 13 Batch 200 Loss 0.0957\n",
      "Epoch 13 Batch 300 Loss 0.0640\n",
      "Epoch 13 Batch 400 Loss 0.0836\n",
      "Epoch 13 Batch 500 Loss 0.0309\n",
      "Epoch 13 Batch 600 Loss 0.0476\n",
      "Epoch 13 Batch 700 Loss 0.0612\n",
      "Epoch 13 Batch 800 Loss 0.0466\n",
      "Epoch 13 Batch 900 Loss 0.0716\n",
      "Epoch 13 Batch 1000 Loss 0.0743\n",
      "Epoch 13 Batch 1100 Loss 0.0926\n",
      "Epoch 13 Batch 1200 Loss 0.0640\n",
      "Epoch 13 Batch 1300 Loss 0.0701\n",
      "Epoch 13 Batch 1400 Loss 0.0694\n",
      "Epoch 13 Batch 1500 Loss 0.0634\n",
      "Epoch 13 Batch 1600 Loss 0.0598\n",
      "Epoch 13 Batch 1700 Loss 0.0855\n",
      "Epoch 13 Batch 1800 Loss 0.1157\n",
      "Epoch 13 Batch 1900 Loss 0.0575\n",
      "Epoch 13 Batch 2000 Loss 0.0794\n",
      "Epoch 13 Batch 2100 Loss 0.1005\n",
      "Epoch 13 Batch 2200 Loss 0.1119\n",
      "Epoch 13 Batch 2300 Loss 0.0707\n",
      "Epoch 13 Batch 2400 Loss 0.0933\n",
      "Epoch 13 Loss 0.0705\n",
      "Time taken for 1 epoch 157.55912971496582 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0596\n",
      "Epoch 14 Batch 100 Loss 0.0568\n",
      "Epoch 14 Batch 200 Loss 0.0498\n",
      "Epoch 14 Batch 300 Loss 0.0404\n",
      "Epoch 14 Batch 400 Loss 0.0438\n",
      "Epoch 14 Batch 500 Loss 0.0471\n",
      "Epoch 14 Batch 600 Loss 0.0456\n",
      "Epoch 14 Batch 700 Loss 0.0677\n",
      "Epoch 14 Batch 800 Loss 0.0947\n",
      "Epoch 14 Batch 900 Loss 0.0648\n",
      "Epoch 14 Batch 1000 Loss 0.0295\n",
      "Epoch 14 Batch 1100 Loss 0.0755\n",
      "Epoch 14 Batch 1200 Loss 0.0630\n",
      "Epoch 14 Batch 1300 Loss 0.0412\n",
      "Epoch 14 Batch 1400 Loss 0.0625\n",
      "Epoch 14 Batch 1500 Loss 0.0955\n",
      "Epoch 14 Batch 1600 Loss 0.0629\n",
      "Epoch 14 Batch 1700 Loss 0.0618\n",
      "Epoch 14 Batch 1800 Loss 0.0752\n",
      "Epoch 14 Batch 1900 Loss 0.0836\n",
      "Epoch 14 Batch 2000 Loss 0.0687\n",
      "Epoch 14 Batch 2100 Loss 0.0937\n",
      "Epoch 14 Batch 2200 Loss 0.0877\n",
      "Epoch 14 Batch 2300 Loss 0.0840\n",
      "Epoch 14 Batch 2400 Loss 0.0765\n",
      "Epoch 14 Loss 0.0675\n",
      "Time taken for 1 epoch 157.47507905960083 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0290\n",
      "Epoch 15 Batch 100 Loss 0.0546\n",
      "Epoch 15 Batch 200 Loss 0.0508\n",
      "Epoch 15 Batch 300 Loss 0.0461\n",
      "Epoch 15 Batch 400 Loss 0.0465\n",
      "Epoch 15 Batch 500 Loss 0.0303\n",
      "Epoch 15 Batch 600 Loss 0.0387\n",
      "Epoch 15 Batch 700 Loss 0.0643\n",
      "Epoch 15 Batch 800 Loss 0.0679\n",
      "Epoch 15 Batch 900 Loss 0.0362\n",
      "Epoch 15 Batch 1000 Loss 0.1241\n",
      "Epoch 15 Batch 1100 Loss 0.0748\n",
      "Epoch 15 Batch 1200 Loss 0.0853\n",
      "Epoch 15 Batch 1300 Loss 0.0885\n",
      "Epoch 15 Batch 1400 Loss 0.0766\n",
      "Epoch 15 Batch 1500 Loss 0.0489\n",
      "Epoch 15 Batch 1600 Loss 0.1002\n",
      "Epoch 15 Batch 1700 Loss 0.0588\n",
      "Epoch 15 Batch 1800 Loss 0.0518\n",
      "Epoch 15 Batch 1900 Loss 0.0574\n",
      "Epoch 15 Batch 2000 Loss 0.0544\n",
      "Epoch 15 Batch 2100 Loss 0.0485\n",
      "Epoch 15 Batch 2200 Loss 0.0813\n",
      "Epoch 15 Batch 2300 Loss 0.0839\n",
      "Epoch 15 Batch 2400 Loss 0.0591\n",
      "Epoch 15 Loss 0.0651\n",
      "Time taken for 1 epoch 157.40972208976746 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0668\n",
      "Epoch 16 Batch 100 Loss 0.0639\n",
      "Epoch 16 Batch 200 Loss 0.0726\n",
      "Epoch 16 Batch 300 Loss 0.0468\n",
      "Epoch 16 Batch 400 Loss 0.0452\n",
      "Epoch 16 Batch 500 Loss 0.0294\n",
      "Epoch 16 Batch 600 Loss 0.0368\n",
      "Epoch 16 Batch 700 Loss 0.0684\n",
      "Epoch 16 Batch 800 Loss 0.0563\n",
      "Epoch 16 Batch 900 Loss 0.0490\n",
      "Epoch 16 Batch 1000 Loss 0.0545\n",
      "Epoch 16 Batch 1100 Loss 0.0681\n",
      "Epoch 16 Batch 1200 Loss 0.0862\n",
      "Epoch 16 Batch 1300 Loss 0.0832\n",
      "Epoch 16 Batch 1400 Loss 0.0731\n",
      "Epoch 16 Batch 1500 Loss 0.0612\n",
      "Epoch 16 Batch 1600 Loss 0.0739\n",
      "Epoch 16 Batch 1700 Loss 0.0888\n",
      "Epoch 16 Batch 1800 Loss 0.0562\n",
      "Epoch 16 Batch 1900 Loss 0.0825\n",
      "Epoch 16 Batch 2000 Loss 0.0614\n",
      "Epoch 16 Batch 2100 Loss 0.0572\n",
      "Epoch 16 Batch 2200 Loss 0.0687\n",
      "Epoch 16 Batch 2300 Loss 0.0777\n",
      "Epoch 16 Batch 2400 Loss 0.0740\n",
      "Epoch 16 Loss 0.0634\n",
      "Time taken for 1 epoch 157.39653134346008 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0517\n",
      "Epoch 17 Batch 100 Loss 0.0471\n",
      "Epoch 17 Batch 200 Loss 0.0571\n",
      "Epoch 17 Batch 300 Loss 0.0585\n",
      "Epoch 17 Batch 400 Loss 0.0460\n",
      "Epoch 17 Batch 500 Loss 0.0658\n",
      "Epoch 17 Batch 600 Loss 0.0552\n",
      "Epoch 17 Batch 700 Loss 0.0418\n",
      "Epoch 17 Batch 800 Loss 0.0321\n",
      "Epoch 17 Batch 900 Loss 0.0594\n",
      "Epoch 17 Batch 1000 Loss 0.0770\n",
      "Epoch 17 Batch 1100 Loss 0.1081\n",
      "Epoch 17 Batch 1200 Loss 0.0627\n",
      "Epoch 17 Batch 1300 Loss 0.0402\n",
      "Epoch 17 Batch 1400 Loss 0.0727\n",
      "Epoch 17 Batch 1500 Loss 0.0580\n",
      "Epoch 17 Batch 1600 Loss 0.0697\n",
      "Epoch 17 Batch 1700 Loss 0.0720\n",
      "Epoch 17 Batch 1800 Loss 0.0592\n",
      "Epoch 17 Batch 1900 Loss 0.0484\n",
      "Epoch 17 Batch 2000 Loss 0.0570\n",
      "Epoch 17 Batch 2100 Loss 0.0839\n",
      "Epoch 17 Batch 2200 Loss 0.0690\n",
      "Epoch 17 Batch 2300 Loss 0.0811\n",
      "Epoch 17 Batch 2400 Loss 0.0458\n",
      "Epoch 17 Loss 0.0622\n",
      "Time taken for 1 epoch 157.38583183288574 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0361\n",
      "Epoch 18 Batch 100 Loss 0.0524\n",
      "Epoch 18 Batch 200 Loss 0.0404\n",
      "Epoch 18 Batch 300 Loss 0.0347\n",
      "Epoch 18 Batch 400 Loss 0.0482\n",
      "Epoch 18 Batch 500 Loss 0.0346\n",
      "Epoch 18 Batch 600 Loss 0.0386\n",
      "Epoch 18 Batch 700 Loss 0.0482\n",
      "Epoch 18 Batch 800 Loss 0.0379\n",
      "Epoch 18 Batch 900 Loss 0.1062\n",
      "Epoch 18 Batch 1000 Loss 0.0621\n",
      "Epoch 18 Batch 1100 Loss 0.0544\n",
      "Epoch 18 Batch 1200 Loss 0.0597\n",
      "Epoch 18 Batch 1300 Loss 0.0480\n",
      "Epoch 18 Batch 1400 Loss 0.0958\n",
      "Epoch 18 Batch 1500 Loss 0.0701\n",
      "Epoch 18 Batch 1600 Loss 0.0836\n",
      "Epoch 18 Batch 1700 Loss 0.0686\n",
      "Epoch 18 Batch 1800 Loss 0.0922\n",
      "Epoch 18 Batch 1900 Loss 0.0706\n",
      "Epoch 18 Batch 2000 Loss 0.0710\n",
      "Epoch 18 Batch 2100 Loss 0.0718\n",
      "Epoch 18 Batch 2200 Loss 0.0766\n",
      "Epoch 18 Batch 2300 Loss 0.0829\n",
      "Epoch 18 Batch 2400 Loss 0.0639\n",
      "Epoch 18 Loss 0.0608\n",
      "Time taken for 1 epoch 157.430757522583 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0499\n",
      "Epoch 19 Batch 100 Loss 0.0453\n",
      "Epoch 19 Batch 200 Loss 0.0400\n",
      "Epoch 19 Batch 300 Loss 0.0332\n",
      "Epoch 19 Batch 400 Loss 0.0451\n",
      "Epoch 19 Batch 500 Loss 0.0382\n",
      "Epoch 19 Batch 600 Loss 0.0717\n",
      "Epoch 19 Batch 700 Loss 0.0449\n",
      "Epoch 19 Batch 800 Loss 0.0624\n",
      "Epoch 19 Batch 900 Loss 0.0700\n",
      "Epoch 19 Batch 1000 Loss 0.0850\n",
      "Epoch 19 Batch 1100 Loss 0.0542\n",
      "Epoch 19 Batch 1200 Loss 0.0449\n",
      "Epoch 19 Batch 1300 Loss 0.0795\n",
      "Epoch 19 Batch 1400 Loss 0.0803\n",
      "Epoch 19 Batch 1500 Loss 0.0726\n",
      "Epoch 19 Batch 1600 Loss 0.0771\n",
      "Epoch 19 Batch 1700 Loss 0.0697\n",
      "Epoch 19 Batch 1800 Loss 0.0621\n",
      "Epoch 19 Batch 1900 Loss 0.0479\n",
      "Epoch 19 Batch 2000 Loss 0.0505\n",
      "Epoch 19 Batch 2100 Loss 0.0472\n",
      "Epoch 19 Batch 2200 Loss 0.0688\n",
      "Epoch 19 Batch 2300 Loss 0.0610\n",
      "Epoch 19 Batch 2400 Loss 0.0660\n",
      "Epoch 19 Loss 0.0588\n",
      "Time taken for 1 epoch 157.3720223903656 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0612\n",
      "Epoch 20 Batch 100 Loss 0.0375\n",
      "Epoch 20 Batch 200 Loss 0.0292\n",
      "Epoch 20 Batch 300 Loss 0.0388\n",
      "Epoch 20 Batch 400 Loss 0.0453\n",
      "Epoch 20 Batch 500 Loss 0.0351\n",
      "Epoch 20 Batch 600 Loss 0.0716\n",
      "Epoch 20 Batch 700 Loss 0.0431\n",
      "Epoch 20 Batch 800 Loss 0.0458\n",
      "Epoch 20 Batch 900 Loss 0.0300\n",
      "Epoch 20 Batch 1000 Loss 0.0530\n",
      "Epoch 20 Batch 1100 Loss 0.0594\n",
      "Epoch 20 Batch 1200 Loss 0.0579\n",
      "Epoch 20 Batch 1300 Loss 0.0557\n",
      "Epoch 20 Batch 1400 Loss 0.0891\n",
      "Epoch 20 Batch 1500 Loss 0.0483\n",
      "Epoch 20 Batch 1600 Loss 0.0613\n",
      "Epoch 20 Batch 1700 Loss 0.0485\n",
      "Epoch 20 Batch 1800 Loss 0.0819\n",
      "Epoch 20 Batch 1900 Loss 0.0578\n",
      "Epoch 20 Batch 2000 Loss 0.1300\n",
      "Epoch 20 Batch 2100 Loss 0.0898\n",
      "Epoch 20 Batch 2200 Loss 0.0902\n",
      "Epoch 20 Batch 2300 Loss 0.0626\n",
      "Epoch 20 Batch 2400 Loss 0.0436\n",
      "Epoch 20 Loss 0.0580\n",
      "Time taken for 1 epoch 157.49798583984375 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0461\n",
      "Epoch 21 Batch 100 Loss 0.0666\n",
      "Epoch 21 Batch 200 Loss 0.0599\n",
      "Epoch 21 Batch 300 Loss 0.0452\n",
      "Epoch 21 Batch 400 Loss 0.0468\n",
      "Epoch 21 Batch 500 Loss 0.0399\n",
      "Epoch 21 Batch 600 Loss 0.0667\n",
      "Epoch 21 Batch 700 Loss 0.0377\n",
      "Epoch 21 Batch 800 Loss 0.0846\n",
      "Epoch 21 Batch 900 Loss 0.0366\n",
      "Epoch 21 Batch 1000 Loss 0.0751\n",
      "Epoch 21 Batch 1100 Loss 0.0608\n",
      "Epoch 21 Batch 1200 Loss 0.0980\n",
      "Epoch 21 Batch 1300 Loss 0.0814\n",
      "Epoch 21 Batch 1400 Loss 0.0671\n",
      "Epoch 21 Batch 1500 Loss 0.0839\n",
      "Epoch 21 Batch 1600 Loss 0.0517\n",
      "Epoch 21 Batch 1700 Loss 0.1040\n",
      "Epoch 21 Batch 1800 Loss 0.0823\n",
      "Epoch 21 Batch 1900 Loss 0.0517\n",
      "Epoch 21 Batch 2000 Loss 0.0571\n",
      "Epoch 21 Batch 2100 Loss 0.0649\n",
      "Epoch 21 Batch 2200 Loss 0.0830\n",
      "Epoch 21 Batch 2300 Loss 0.0629\n",
      "Epoch 21 Batch 2400 Loss 0.0946\n",
      "Epoch 21 Loss 0.0570\n",
      "Time taken for 1 epoch 157.54207587242126 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0351\n",
      "Epoch 22 Batch 100 Loss 0.0478\n",
      "Epoch 22 Batch 200 Loss 0.0374\n",
      "Epoch 22 Batch 300 Loss 0.0428\n",
      "Epoch 22 Batch 400 Loss 0.0401\n",
      "Epoch 22 Batch 500 Loss 0.0623\n",
      "Epoch 22 Batch 600 Loss 0.0594\n",
      "Epoch 22 Batch 700 Loss 0.0582\n",
      "Epoch 22 Batch 800 Loss 0.0464\n",
      "Epoch 22 Batch 900 Loss 0.0476\n",
      "Epoch 22 Batch 1000 Loss 0.0652\n",
      "Epoch 22 Batch 1100 Loss 0.0433\n",
      "Epoch 22 Batch 1200 Loss 0.0674\n",
      "Epoch 22 Batch 1300 Loss 0.0604\n",
      "Epoch 22 Batch 1400 Loss 0.0518\n",
      "Epoch 22 Batch 1500 Loss 0.0560\n",
      "Epoch 22 Batch 1600 Loss 0.0555\n",
      "Epoch 22 Batch 1700 Loss 0.0744\n",
      "Epoch 22 Batch 1800 Loss 0.0927\n",
      "Epoch 22 Batch 1900 Loss 0.0901\n",
      "Epoch 22 Batch 2000 Loss 0.0930\n",
      "Epoch 22 Batch 2100 Loss 0.0390\n",
      "Epoch 22 Batch 2200 Loss 0.0488\n",
      "Epoch 22 Batch 2300 Loss 0.0656\n",
      "Epoch 22 Batch 2400 Loss 0.0667\n",
      "Epoch 22 Loss 0.0559\n",
      "Time taken for 1 epoch 157.60302305221558 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0462\n",
      "Epoch 23 Batch 100 Loss 0.0415\n",
      "Epoch 23 Batch 200 Loss 0.0441\n",
      "Epoch 23 Batch 300 Loss 0.0566\n",
      "Epoch 23 Batch 400 Loss 0.0316\n",
      "Epoch 23 Batch 500 Loss 0.0297\n",
      "Epoch 23 Batch 600 Loss 0.0572\n",
      "Epoch 23 Batch 700 Loss 0.0422\n",
      "Epoch 23 Batch 800 Loss 0.0589\n",
      "Epoch 23 Batch 900 Loss 0.0439\n",
      "Epoch 23 Batch 1000 Loss 0.0534\n",
      "Epoch 23 Batch 1100 Loss 0.0941\n",
      "Epoch 23 Batch 1200 Loss 0.0680\n",
      "Epoch 23 Batch 1300 Loss 0.0599\n",
      "Epoch 23 Batch 1400 Loss 0.0889\n",
      "Epoch 23 Batch 1500 Loss 0.0418\n",
      "Epoch 23 Batch 1600 Loss 0.0515\n",
      "Epoch 23 Batch 1700 Loss 0.0529\n",
      "Epoch 23 Batch 1800 Loss 0.0709\n",
      "Epoch 23 Batch 1900 Loss 0.0532\n",
      "Epoch 23 Batch 2000 Loss 0.0839\n",
      "Epoch 23 Batch 2100 Loss 0.0912\n",
      "Epoch 23 Batch 2200 Loss 0.0464\n",
      "Epoch 23 Batch 2300 Loss 0.0483\n",
      "Epoch 23 Batch 2400 Loss 0.0695\n",
      "Epoch 23 Loss 0.0551\n",
      "Time taken for 1 epoch 157.38504695892334 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0328\n",
      "Epoch 24 Batch 100 Loss 0.0359\n",
      "Epoch 24 Batch 200 Loss 0.0569\n",
      "Epoch 24 Batch 300 Loss 0.0245\n",
      "Epoch 24 Batch 400 Loss 0.0807\n",
      "Epoch 24 Batch 500 Loss 0.0733\n",
      "Epoch 24 Batch 600 Loss 0.0367\n",
      "Epoch 24 Batch 700 Loss 0.0377\n",
      "Epoch 24 Batch 800 Loss 0.0646\n",
      "Epoch 24 Batch 900 Loss 0.0397\n",
      "Epoch 24 Batch 1000 Loss 0.0537\n",
      "Epoch 24 Batch 1100 Loss 0.0405\n",
      "Epoch 24 Batch 1200 Loss 0.0516\n",
      "Epoch 24 Batch 1300 Loss 0.0705\n",
      "Epoch 24 Batch 1400 Loss 0.0516\n",
      "Epoch 24 Batch 1500 Loss 0.0399\n",
      "Epoch 24 Batch 1600 Loss 0.0723\n",
      "Epoch 24 Batch 1700 Loss 0.0638\n",
      "Epoch 24 Batch 1800 Loss 0.0885\n",
      "Epoch 24 Batch 1900 Loss 0.0802\n",
      "Epoch 24 Batch 2000 Loss 0.0845\n",
      "Epoch 24 Batch 2100 Loss 0.0563\n",
      "Epoch 24 Batch 2200 Loss 0.0496\n",
      "Epoch 24 Batch 2300 Loss 0.0728\n",
      "Epoch 24 Batch 2400 Loss 0.0575\n",
      "Epoch 24 Loss 0.0545\n",
      "Time taken for 1 epoch 157.66093134880066 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0485\n",
      "Epoch 25 Batch 100 Loss 0.0307\n",
      "Epoch 25 Batch 200 Loss 0.0629\n",
      "Epoch 25 Batch 300 Loss 0.0511\n",
      "Epoch 25 Batch 400 Loss 0.0396\n",
      "Epoch 25 Batch 500 Loss 0.0429\n",
      "Epoch 25 Batch 600 Loss 0.0532\n",
      "Epoch 25 Batch 700 Loss 0.0613\n",
      "Epoch 25 Batch 800 Loss 0.0821\n",
      "Epoch 25 Batch 900 Loss 0.0986\n",
      "Epoch 25 Batch 1000 Loss 0.0580\n",
      "Epoch 25 Batch 1100 Loss 0.0487\n",
      "Epoch 25 Batch 1200 Loss 0.0692\n",
      "Epoch 25 Batch 1300 Loss 0.0546\n",
      "Epoch 25 Batch 1400 Loss 0.0488\n",
      "Epoch 25 Batch 1500 Loss 0.0561\n",
      "Epoch 25 Batch 1600 Loss 0.0538\n",
      "Epoch 25 Batch 1700 Loss 0.0719\n",
      "Epoch 25 Batch 1800 Loss 0.0540\n",
      "Epoch 25 Batch 1900 Loss 0.0869\n",
      "Epoch 25 Batch 2000 Loss 0.0362\n",
      "Epoch 25 Batch 2100 Loss 0.0614\n",
      "Epoch 25 Batch 2200 Loss 0.0478\n",
      "Epoch 25 Batch 2300 Loss 0.0505\n",
      "Epoch 25 Batch 2400 Loss 0.0977\n",
      "Epoch 25 Loss 0.0539\n",
      "Time taken for 1 epoch 158.15268230438232 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0419\n",
      "Epoch 26 Batch 100 Loss 0.0346\n",
      "Epoch 26 Batch 200 Loss 0.0361\n",
      "Epoch 26 Batch 300 Loss 0.0280\n",
      "Epoch 26 Batch 400 Loss 0.0496\n",
      "Epoch 26 Batch 500 Loss 0.0615\n",
      "Epoch 26 Batch 600 Loss 0.0479\n",
      "Epoch 26 Batch 700 Loss 0.0538\n",
      "Epoch 26 Batch 800 Loss 0.0337\n",
      "Epoch 26 Batch 900 Loss 0.0655\n",
      "Epoch 26 Batch 1000 Loss 0.0331\n",
      "Epoch 26 Batch 1100 Loss 0.0456\n",
      "Epoch 26 Batch 1200 Loss 0.0419\n",
      "Epoch 26 Batch 1300 Loss 0.0715\n",
      "Epoch 26 Batch 1400 Loss 0.0535\n",
      "Epoch 26 Batch 1500 Loss 0.0558\n",
      "Epoch 26 Batch 1600 Loss 0.0828\n",
      "Epoch 26 Batch 1700 Loss 0.0404\n",
      "Epoch 26 Batch 1800 Loss 0.0599\n",
      "Epoch 26 Batch 1900 Loss 0.0546\n",
      "Epoch 26 Batch 2000 Loss 0.0429\n",
      "Epoch 26 Batch 2100 Loss 0.0305\n",
      "Epoch 26 Batch 2200 Loss 0.0765\n",
      "Epoch 26 Batch 2300 Loss 0.0693\n",
      "Epoch 26 Batch 2400 Loss 0.0974\n",
      "Epoch 26 Loss 0.0527\n",
      "Time taken for 1 epoch 157.52503871917725 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0429\n",
      "Epoch 27 Batch 100 Loss 0.0329\n",
      "Epoch 27 Batch 200 Loss 0.0354\n",
      "Epoch 27 Batch 300 Loss 0.0632\n",
      "Epoch 27 Batch 400 Loss 0.0408\n",
      "Epoch 27 Batch 500 Loss 0.0822\n",
      "Epoch 27 Batch 600 Loss 0.0425\n",
      "Epoch 27 Batch 700 Loss 0.0442\n",
      "Epoch 27 Batch 800 Loss 0.0452\n",
      "Epoch 27 Batch 900 Loss 0.0359\n",
      "Epoch 27 Batch 1000 Loss 0.0722\n",
      "Epoch 27 Batch 1100 Loss 0.0285\n",
      "Epoch 27 Batch 1200 Loss 0.0565\n",
      "Epoch 27 Batch 1300 Loss 0.0481\n",
      "Epoch 27 Batch 1400 Loss 0.0432\n",
      "Epoch 27 Batch 1500 Loss 0.0677\n",
      "Epoch 27 Batch 1600 Loss 0.0534\n",
      "Epoch 27 Batch 1700 Loss 0.0399\n",
      "Epoch 27 Batch 1800 Loss 0.0281\n",
      "Epoch 27 Batch 1900 Loss 0.0428\n",
      "Epoch 27 Batch 2000 Loss 0.0477\n",
      "Epoch 27 Batch 2100 Loss 0.0413\n",
      "Epoch 27 Batch 2200 Loss 0.0332\n",
      "Epoch 27 Batch 2300 Loss 0.0748\n",
      "Epoch 27 Batch 2400 Loss 0.0552\n",
      "Epoch 27 Loss 0.0525\n",
      "Time taken for 1 epoch 157.56306648254395 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0448\n",
      "Epoch 28 Batch 100 Loss 0.0329\n",
      "Epoch 28 Batch 200 Loss 0.0712\n",
      "Epoch 28 Batch 300 Loss 0.0503\n",
      "Epoch 28 Batch 400 Loss 0.0333\n",
      "Epoch 28 Batch 500 Loss 0.0397\n",
      "Epoch 28 Batch 600 Loss 0.0378\n",
      "Epoch 28 Batch 700 Loss 0.0529\n",
      "Epoch 28 Batch 800 Loss 0.0390\n",
      "Epoch 28 Batch 900 Loss 0.0372\n",
      "Epoch 28 Batch 1000 Loss 0.0258\n",
      "Epoch 28 Batch 1100 Loss 0.0543\n",
      "Epoch 28 Batch 1200 Loss 0.0559\n",
      "Epoch 28 Batch 1300 Loss 0.0588\n",
      "Epoch 28 Batch 1400 Loss 0.0501\n",
      "Epoch 28 Batch 1500 Loss 0.0544\n",
      "Epoch 28 Batch 1600 Loss 0.0530\n",
      "Epoch 28 Batch 1700 Loss 0.0425\n",
      "Epoch 28 Batch 1800 Loss 0.0779\n",
      "Epoch 28 Batch 1900 Loss 0.0511\n",
      "Epoch 28 Batch 2000 Loss 0.0798\n",
      "Epoch 28 Batch 2100 Loss 0.0788\n",
      "Epoch 28 Batch 2200 Loss 0.0243\n",
      "Epoch 28 Batch 2300 Loss 0.0616\n",
      "Epoch 28 Batch 2400 Loss 0.0391\n",
      "Epoch 28 Loss 0.0515\n",
      "Time taken for 1 epoch 157.3078179359436 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0603\n",
      "Epoch 29 Batch 100 Loss 0.0412\n",
      "Epoch 29 Batch 200 Loss 0.0234\n",
      "Epoch 29 Batch 300 Loss 0.0386\n",
      "Epoch 29 Batch 400 Loss 0.0329\n",
      "Epoch 29 Batch 500 Loss 0.0655\n",
      "Epoch 29 Batch 600 Loss 0.0519\n",
      "Epoch 29 Batch 700 Loss 0.0380\n",
      "Epoch 29 Batch 800 Loss 0.0400\n",
      "Epoch 29 Batch 900 Loss 0.0750\n",
      "Epoch 29 Batch 1000 Loss 0.0276\n",
      "Epoch 29 Batch 1100 Loss 0.0642\n",
      "Epoch 29 Batch 1200 Loss 0.0409\n",
      "Epoch 29 Batch 1300 Loss 0.0305\n",
      "Epoch 29 Batch 1400 Loss 0.0510\n",
      "Epoch 29 Batch 1500 Loss 0.0576\n",
      "Epoch 29 Batch 1600 Loss 0.0644\n",
      "Epoch 29 Batch 1700 Loss 0.0752\n",
      "Epoch 29 Batch 1800 Loss 0.0704\n",
      "Epoch 29 Batch 1900 Loss 0.0621\n",
      "Epoch 29 Batch 2000 Loss 0.0618\n",
      "Epoch 29 Batch 2100 Loss 0.0603\n",
      "Epoch 29 Batch 2200 Loss 0.0431\n",
      "Epoch 29 Batch 2300 Loss 0.0561\n",
      "Epoch 29 Batch 2400 Loss 0.0634\n",
      "Epoch 29 Loss 0.0510\n",
      "Time taken for 1 epoch 157.5125184059143 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0213\n",
      "Epoch 30 Batch 100 Loss 0.0343\n",
      "Epoch 30 Batch 200 Loss 0.0504\n",
      "Epoch 30 Batch 300 Loss 0.0596\n",
      "Epoch 30 Batch 400 Loss 0.0718\n",
      "Epoch 30 Batch 500 Loss 0.0273\n",
      "Epoch 30 Batch 600 Loss 0.0318\n",
      "Epoch 30 Batch 700 Loss 0.0308\n",
      "Epoch 30 Batch 800 Loss 0.0524\n",
      "Epoch 30 Batch 900 Loss 0.0295\n",
      "Epoch 30 Batch 1000 Loss 0.0706\n",
      "Epoch 30 Batch 1100 Loss 0.0320\n",
      "Epoch 30 Batch 1200 Loss 0.0720\n",
      "Epoch 30 Batch 1300 Loss 0.0608\n",
      "Epoch 30 Batch 1400 Loss 0.0558\n",
      "Epoch 30 Batch 1500 Loss 0.0166\n",
      "Epoch 30 Batch 1600 Loss 0.0510\n",
      "Epoch 30 Batch 1700 Loss 0.0382\n",
      "Epoch 30 Batch 1800 Loss 0.0750\n",
      "Epoch 30 Batch 1900 Loss 0.0548\n",
      "Epoch 30 Batch 2000 Loss 0.0756\n",
      "Epoch 30 Batch 2100 Loss 0.0353\n",
      "Epoch 30 Batch 2200 Loss 0.0432\n",
      "Epoch 30 Batch 2300 Loss 0.0594\n",
      "Epoch 30 Batch 2400 Loss 0.0728\n",
      "Epoch 30 Loss 0.0504\n",
      "Time taken for 1 epoch 157.2838146686554 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0336\n",
      "Epoch 31 Batch 100 Loss 0.0292\n",
      "Epoch 31 Batch 200 Loss 0.0350\n",
      "Epoch 31 Batch 300 Loss 0.0338\n",
      "Epoch 31 Batch 400 Loss 0.0155\n",
      "Epoch 31 Batch 500 Loss 0.0474\n",
      "Epoch 31 Batch 600 Loss 0.0177\n",
      "Epoch 31 Batch 700 Loss 0.0396\n",
      "Epoch 31 Batch 800 Loss 0.0534\n",
      "Epoch 31 Batch 900 Loss 0.0508\n",
      "Epoch 31 Batch 1000 Loss 0.0423\n",
      "Epoch 31 Batch 1100 Loss 0.0209\n",
      "Epoch 31 Batch 1200 Loss 0.0387\n",
      "Epoch 31 Batch 1300 Loss 0.0554\n",
      "Epoch 31 Batch 1400 Loss 0.0556\n",
      "Epoch 31 Batch 1500 Loss 0.0418\n",
      "Epoch 31 Batch 1600 Loss 0.0659\n",
      "Epoch 31 Batch 1700 Loss 0.0471\n",
      "Epoch 31 Batch 1800 Loss 0.0552\n",
      "Epoch 31 Batch 1900 Loss 0.0641\n",
      "Epoch 31 Batch 2000 Loss 0.0611\n",
      "Epoch 31 Batch 2100 Loss 0.0831\n",
      "Epoch 31 Batch 2200 Loss 0.0215\n",
      "Epoch 31 Batch 2300 Loss 0.0754\n",
      "Epoch 31 Batch 2400 Loss 0.0435\n",
      "Epoch 31 Loss 0.0503\n",
      "Time taken for 1 epoch 157.2540521621704 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0271\n",
      "Epoch 32 Batch 100 Loss 0.0610\n",
      "Epoch 32 Batch 200 Loss 0.0471\n",
      "Epoch 32 Batch 300 Loss 0.0696\n",
      "Epoch 32 Batch 400 Loss 0.0431\n",
      "Epoch 32 Batch 500 Loss 0.0308\n",
      "Epoch 32 Batch 600 Loss 0.0514\n",
      "Epoch 32 Batch 700 Loss 0.0458\n",
      "Epoch 32 Batch 800 Loss 0.0731\n",
      "Epoch 32 Batch 900 Loss 0.0515\n",
      "Epoch 32 Batch 1000 Loss 0.0672\n",
      "Epoch 32 Batch 1100 Loss 0.0519\n",
      "Epoch 32 Batch 1200 Loss 0.0455\n",
      "Epoch 32 Batch 1300 Loss 0.0811\n",
      "Epoch 32 Batch 1400 Loss 0.0385\n",
      "Epoch 32 Batch 1500 Loss 0.0504\n",
      "Epoch 32 Batch 1600 Loss 0.0762\n",
      "Epoch 32 Batch 1700 Loss 0.0441\n",
      "Epoch 32 Batch 1800 Loss 0.0569\n",
      "Epoch 32 Batch 1900 Loss 0.0205\n",
      "Epoch 32 Batch 2000 Loss 0.0652\n",
      "Epoch 32 Batch 2100 Loss 0.0319\n",
      "Epoch 32 Batch 2200 Loss 0.0538\n",
      "Epoch 32 Batch 2300 Loss 0.0763\n",
      "Epoch 32 Batch 2400 Loss 0.0335\n",
      "Epoch 32 Loss 0.0497\n",
      "Time taken for 1 epoch 157.4278736114502 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0749\n",
      "Epoch 33 Batch 100 Loss 0.0600\n",
      "Epoch 33 Batch 200 Loss 0.0399\n",
      "Epoch 33 Batch 300 Loss 0.0292\n",
      "Epoch 33 Batch 400 Loss 0.0573\n",
      "Epoch 33 Batch 500 Loss 0.0351\n",
      "Epoch 33 Batch 600 Loss 0.0440\n",
      "Epoch 33 Batch 700 Loss 0.0448\n",
      "Epoch 33 Batch 800 Loss 0.0604\n",
      "Epoch 33 Batch 900 Loss 0.0392\n",
      "Epoch 33 Batch 1000 Loss 0.0357\n",
      "Epoch 33 Batch 1100 Loss 0.0613\n",
      "Epoch 33 Batch 1200 Loss 0.0394\n",
      "Epoch 33 Batch 1300 Loss 0.0572\n",
      "Epoch 33 Batch 1400 Loss 0.0615\n",
      "Epoch 33 Batch 1500 Loss 0.0692\n",
      "Epoch 33 Batch 1600 Loss 0.0518\n",
      "Epoch 33 Batch 1700 Loss 0.0555\n",
      "Epoch 33 Batch 1800 Loss 0.0474\n",
      "Epoch 33 Batch 1900 Loss 0.0415\n",
      "Epoch 33 Batch 2000 Loss 0.0527\n",
      "Epoch 33 Batch 2100 Loss 0.0556\n",
      "Epoch 33 Batch 2200 Loss 0.0792\n",
      "Epoch 33 Batch 2300 Loss 0.0571\n",
      "Epoch 33 Batch 2400 Loss 0.0477\n",
      "Epoch 33 Loss 0.0494\n",
      "Time taken for 1 epoch 157.4268844127655 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0549\n",
      "Epoch 34 Batch 100 Loss 0.0479\n",
      "Epoch 34 Batch 200 Loss 0.0204\n",
      "Epoch 34 Batch 300 Loss 0.0298\n",
      "Epoch 34 Batch 400 Loss 0.0568\n",
      "Epoch 34 Batch 500 Loss 0.0728\n",
      "Epoch 34 Batch 600 Loss 0.0544\n",
      "Epoch 34 Batch 700 Loss 0.0688\n",
      "Epoch 34 Batch 800 Loss 0.0594\n",
      "Epoch 34 Batch 900 Loss 0.0425\n",
      "Epoch 34 Batch 1000 Loss 0.0621\n",
      "Epoch 34 Batch 1100 Loss 0.0403\n",
      "Epoch 34 Batch 1200 Loss 0.0309\n",
      "Epoch 34 Batch 1300 Loss 0.0492\n",
      "Epoch 34 Batch 1400 Loss 0.0525\n",
      "Epoch 34 Batch 1500 Loss 0.0331\n",
      "Epoch 34 Batch 1600 Loss 0.0582\n",
      "Epoch 34 Batch 1700 Loss 0.0597\n",
      "Epoch 34 Batch 1800 Loss 0.0540\n",
      "Epoch 34 Batch 1900 Loss 0.0664\n",
      "Epoch 34 Batch 2000 Loss 0.0559\n",
      "Epoch 34 Batch 2100 Loss 0.0548\n",
      "Epoch 34 Batch 2200 Loss 0.0491\n",
      "Epoch 34 Batch 2300 Loss 0.0831\n",
      "Epoch 34 Batch 2400 Loss 0.0768\n",
      "Epoch 34 Loss 0.0488\n",
      "Time taken for 1 epoch 157.33058214187622 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0480\n",
      "Epoch 35 Batch 100 Loss 0.0226\n",
      "Epoch 35 Batch 200 Loss 0.0378\n",
      "Epoch 35 Batch 300 Loss 0.0264\n",
      "Epoch 35 Batch 400 Loss 0.0239\n",
      "Epoch 35 Batch 500 Loss 0.0430\n",
      "Epoch 35 Batch 600 Loss 0.0501\n",
      "Epoch 35 Batch 700 Loss 0.0452\n",
      "Epoch 35 Batch 800 Loss 0.0462\n",
      "Epoch 35 Batch 900 Loss 0.0517\n",
      "Epoch 35 Batch 1000 Loss 0.0542\n",
      "Epoch 35 Batch 1100 Loss 0.0640\n",
      "Epoch 35 Batch 1200 Loss 0.0310\n",
      "Epoch 35 Batch 1300 Loss 0.0479\n",
      "Epoch 35 Batch 1400 Loss 0.0464\n",
      "Epoch 35 Batch 1500 Loss 0.0949\n",
      "Epoch 35 Batch 1600 Loss 0.0502\n",
      "Epoch 35 Batch 1700 Loss 0.0442\n",
      "Epoch 35 Batch 1800 Loss 0.0790\n",
      "Epoch 35 Batch 1900 Loss 0.0415\n",
      "Epoch 35 Batch 2000 Loss 0.0399\n",
      "Epoch 35 Batch 2100 Loss 0.0420\n",
      "Epoch 35 Batch 2200 Loss 0.0474\n",
      "Epoch 35 Batch 2300 Loss 0.0381\n",
      "Epoch 35 Batch 2400 Loss 0.0647\n",
      "Epoch 35 Loss 0.0485\n",
      "Time taken for 1 epoch 157.74024510383606 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0271\n",
      "Epoch 36 Batch 100 Loss 0.0485\n",
      "Epoch 36 Batch 200 Loss 0.0461\n",
      "Epoch 36 Batch 300 Loss 0.0513\n",
      "Epoch 36 Batch 400 Loss 0.0292\n",
      "Epoch 36 Batch 500 Loss 0.0339\n",
      "Epoch 36 Batch 600 Loss 0.0331\n",
      "Epoch 36 Batch 700 Loss 0.0401\n",
      "Epoch 36 Batch 800 Loss 0.0362\n",
      "Epoch 36 Batch 900 Loss 0.0455\n",
      "Epoch 36 Batch 1000 Loss 0.0434\n",
      "Epoch 36 Batch 1100 Loss 0.0333\n",
      "Epoch 36 Batch 1200 Loss 0.0416\n",
      "Epoch 36 Batch 1300 Loss 0.0600\n",
      "Epoch 36 Batch 1400 Loss 0.0336\n",
      "Epoch 36 Batch 1500 Loss 0.0631\n",
      "Epoch 36 Batch 1600 Loss 0.0561\n",
      "Epoch 36 Batch 1700 Loss 0.0876\n",
      "Epoch 36 Batch 1800 Loss 0.0570\n",
      "Epoch 36 Batch 1900 Loss 0.0356\n",
      "Epoch 36 Batch 2000 Loss 0.0474\n",
      "Epoch 36 Batch 2100 Loss 0.0335\n",
      "Epoch 36 Batch 2200 Loss 0.0475\n",
      "Epoch 36 Batch 2300 Loss 0.0658\n",
      "Epoch 36 Batch 2400 Loss 0.0443\n",
      "Epoch 36 Loss 0.0481\n",
      "Time taken for 1 epoch 157.29515504837036 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0258\n",
      "Epoch 37 Batch 100 Loss 0.0502\n",
      "Epoch 37 Batch 200 Loss 0.0296\n",
      "Epoch 37 Batch 300 Loss 0.0353\n",
      "Epoch 37 Batch 400 Loss 0.0505\n",
      "Epoch 37 Batch 500 Loss 0.0374\n",
      "Epoch 37 Batch 600 Loss 0.0435\n",
      "Epoch 37 Batch 700 Loss 0.0466\n",
      "Epoch 37 Batch 800 Loss 0.0629\n",
      "Epoch 37 Batch 900 Loss 0.0330\n",
      "Epoch 37 Batch 1000 Loss 0.0417\n",
      "Epoch 37 Batch 1100 Loss 0.0347\n",
      "Epoch 37 Batch 1200 Loss 0.0286\n",
      "Epoch 37 Batch 1300 Loss 0.0328\n",
      "Epoch 37 Batch 1400 Loss 0.0584\n",
      "Epoch 37 Batch 1500 Loss 0.0284\n",
      "Epoch 37 Batch 1600 Loss 0.0262\n",
      "Epoch 37 Batch 1700 Loss 0.0622\n",
      "Epoch 37 Batch 1800 Loss 0.0600\n",
      "Epoch 37 Batch 1900 Loss 0.0490\n",
      "Epoch 37 Batch 2000 Loss 0.0420\n",
      "Epoch 37 Batch 2100 Loss 0.0507\n",
      "Epoch 37 Batch 2200 Loss 0.0616\n",
      "Epoch 37 Batch 2300 Loss 0.0462\n",
      "Epoch 37 Batch 2400 Loss 0.0748\n",
      "Epoch 37 Loss 0.0476\n",
      "Time taken for 1 epoch 157.2373116016388 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0219\n",
      "Epoch 38 Batch 100 Loss 0.0582\n",
      "Epoch 38 Batch 200 Loss 0.0489\n",
      "Epoch 38 Batch 300 Loss 0.0574\n",
      "Epoch 38 Batch 400 Loss 0.0241\n",
      "Epoch 38 Batch 500 Loss 0.0326\n",
      "Epoch 38 Batch 600 Loss 0.0249\n",
      "Epoch 38 Batch 700 Loss 0.0343\n",
      "Epoch 38 Batch 800 Loss 0.0474\n",
      "Epoch 38 Batch 900 Loss 0.0406\n",
      "Epoch 38 Batch 1000 Loss 0.0671\n",
      "Epoch 38 Batch 1100 Loss 0.0473\n",
      "Epoch 38 Batch 1200 Loss 0.0229\n",
      "Epoch 38 Batch 1300 Loss 0.0346\n",
      "Epoch 38 Batch 1400 Loss 0.0579\n",
      "Epoch 38 Batch 1500 Loss 0.0405\n",
      "Epoch 38 Batch 1600 Loss 0.0578\n",
      "Epoch 38 Batch 1700 Loss 0.0469\n",
      "Epoch 38 Batch 1800 Loss 0.0581\n",
      "Epoch 38 Batch 1900 Loss 0.0718\n",
      "Epoch 38 Batch 2000 Loss 0.0590\n",
      "Epoch 38 Batch 2100 Loss 0.0275\n",
      "Epoch 38 Batch 2200 Loss 0.0780\n",
      "Epoch 38 Batch 2300 Loss 0.0646\n",
      "Epoch 38 Batch 2400 Loss 0.0476\n",
      "Epoch 38 Loss 0.0476\n",
      "Time taken for 1 epoch 157.3109209537506 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0406\n",
      "Epoch 39 Batch 100 Loss 0.0260\n",
      "Epoch 39 Batch 200 Loss 0.0396\n",
      "Epoch 39 Batch 300 Loss 0.0407\n",
      "Epoch 39 Batch 400 Loss 0.0514\n",
      "Epoch 39 Batch 500 Loss 0.0624\n",
      "Epoch 39 Batch 600 Loss 0.0585\n",
      "Epoch 39 Batch 700 Loss 0.0387\n",
      "Epoch 39 Batch 800 Loss 0.0560\n",
      "Epoch 39 Batch 900 Loss 0.0617\n",
      "Epoch 39 Batch 1000 Loss 0.0496\n",
      "Epoch 39 Batch 1100 Loss 0.0566\n",
      "Epoch 39 Batch 1200 Loss 0.0442\n",
      "Epoch 39 Batch 1300 Loss 0.0559\n",
      "Epoch 39 Batch 1400 Loss 0.0451\n",
      "Epoch 39 Batch 1500 Loss 0.0660\n",
      "Epoch 39 Batch 1600 Loss 0.0582\n",
      "Epoch 39 Batch 1700 Loss 0.0627\n",
      "Epoch 39 Batch 1800 Loss 0.0585\n",
      "Epoch 39 Batch 1900 Loss 0.0409\n",
      "Epoch 39 Batch 2000 Loss 0.0618\n",
      "Epoch 39 Batch 2100 Loss 0.0722\n",
      "Epoch 39 Batch 2200 Loss 0.0566\n",
      "Epoch 39 Batch 2300 Loss 0.0506\n",
      "Epoch 39 Batch 2400 Loss 0.0513\n",
      "Epoch 39 Loss 0.0473\n",
      "Time taken for 1 epoch 157.32360458374023 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0263\n",
      "Epoch 40 Batch 100 Loss 0.0515\n",
      "Epoch 40 Batch 200 Loss 0.0359\n",
      "Epoch 40 Batch 300 Loss 0.0430\n",
      "Epoch 40 Batch 400 Loss 0.0496\n",
      "Epoch 40 Batch 500 Loss 0.0420\n",
      "Epoch 40 Batch 600 Loss 0.0442\n",
      "Epoch 40 Batch 700 Loss 0.0393\n",
      "Epoch 40 Batch 800 Loss 0.0331\n",
      "Epoch 40 Batch 900 Loss 0.0498\n",
      "Epoch 40 Batch 1000 Loss 0.0645\n",
      "Epoch 40 Batch 1100 Loss 0.0466\n",
      "Epoch 40 Batch 1200 Loss 0.0417\n",
      "Epoch 40 Batch 1300 Loss 0.0619\n",
      "Epoch 40 Batch 1400 Loss 0.0451\n",
      "Epoch 40 Batch 1500 Loss 0.0304\n",
      "Epoch 40 Batch 1600 Loss 0.0314\n",
      "Epoch 40 Batch 1700 Loss 0.0517\n",
      "Epoch 40 Batch 1800 Loss 0.0757\n",
      "Epoch 40 Batch 1900 Loss 0.0718\n",
      "Epoch 40 Batch 2000 Loss 0.0417\n",
      "Epoch 40 Batch 2100 Loss 0.0872\n",
      "Epoch 40 Batch 2200 Loss 0.0673\n",
      "Epoch 40 Batch 2300 Loss 0.0523\n",
      "Epoch 40 Batch 2400 Loss 0.0654\n",
      "Epoch 40 Loss 0.0466\n",
      "Time taken for 1 epoch 157.2469298839569 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0475\n",
      "Epoch 41 Batch 100 Loss 0.0335\n",
      "Epoch 41 Batch 200 Loss 0.0480\n",
      "Epoch 41 Batch 300 Loss 0.0284\n",
      "Epoch 41 Batch 400 Loss 0.0272\n",
      "Epoch 41 Batch 500 Loss 0.0328\n",
      "Epoch 41 Batch 600 Loss 0.0397\n",
      "Epoch 41 Batch 700 Loss 0.0556\n",
      "Epoch 41 Batch 800 Loss 0.0488\n",
      "Epoch 41 Batch 900 Loss 0.0602\n",
      "Epoch 41 Batch 1000 Loss 0.0478\n",
      "Epoch 41 Batch 1100 Loss 0.0273\n",
      "Epoch 41 Batch 1200 Loss 0.0480\n",
      "Epoch 41 Batch 1300 Loss 0.0315\n",
      "Epoch 41 Batch 1400 Loss 0.0493\n",
      "Epoch 41 Batch 1500 Loss 0.0670\n",
      "Epoch 41 Batch 1600 Loss 0.0528\n",
      "Epoch 41 Batch 1700 Loss 0.0763\n",
      "Epoch 41 Batch 1800 Loss 0.0648\n",
      "Epoch 41 Batch 1900 Loss 0.0500\n",
      "Epoch 41 Batch 2000 Loss 0.0469\n",
      "Epoch 41 Batch 2100 Loss 0.0523\n",
      "Epoch 41 Batch 2200 Loss 0.0554\n",
      "Epoch 41 Batch 2300 Loss 0.0448\n",
      "Epoch 41 Batch 2400 Loss 0.0556\n",
      "Epoch 41 Loss 0.0469\n",
      "Time taken for 1 epoch 157.2371063232422 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0285\n",
      "Epoch 42 Batch 100 Loss 0.0531\n",
      "Epoch 42 Batch 200 Loss 0.0395\n",
      "Epoch 42 Batch 300 Loss 0.0294\n",
      "Epoch 42 Batch 400 Loss 0.0707\n",
      "Epoch 42 Batch 500 Loss 0.0358\n",
      "Epoch 42 Batch 600 Loss 0.0277\n",
      "Epoch 42 Batch 700 Loss 0.0486\n",
      "Epoch 42 Batch 800 Loss 0.0285\n",
      "Epoch 42 Batch 900 Loss 0.0263\n",
      "Epoch 42 Batch 1000 Loss 0.0526\n",
      "Epoch 42 Batch 1100 Loss 0.0333\n",
      "Epoch 42 Batch 1200 Loss 0.0181\n",
      "Epoch 42 Batch 1300 Loss 0.0661\n",
      "Epoch 42 Batch 1400 Loss 0.0484\n",
      "Epoch 42 Batch 1500 Loss 0.0669\n",
      "Epoch 42 Batch 1600 Loss 0.0419\n",
      "Epoch 42 Batch 1700 Loss 0.0658\n",
      "Epoch 42 Batch 1800 Loss 0.0624\n",
      "Epoch 42 Batch 1900 Loss 0.0485\n",
      "Epoch 42 Batch 2000 Loss 0.0598\n",
      "Epoch 42 Batch 2100 Loss 0.0439\n",
      "Epoch 42 Batch 2200 Loss 0.0700\n",
      "Epoch 42 Batch 2300 Loss 0.0430\n",
      "Epoch 42 Batch 2400 Loss 0.0605\n",
      "Epoch 42 Loss 0.0463\n",
      "Time taken for 1 epoch 157.4142918586731 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0211\n",
      "Epoch 43 Batch 100 Loss 0.0559\n",
      "Epoch 43 Batch 200 Loss 0.0543\n",
      "Epoch 43 Batch 300 Loss 0.0224\n",
      "Epoch 43 Batch 400 Loss 0.0391\n",
      "Epoch 43 Batch 500 Loss 0.0269\n",
      "Epoch 43 Batch 600 Loss 0.0363\n",
      "Epoch 43 Batch 700 Loss 0.0334\n",
      "Epoch 43 Batch 800 Loss 0.0398\n",
      "Epoch 43 Batch 900 Loss 0.0515\n",
      "Epoch 43 Batch 1000 Loss 0.0535\n",
      "Epoch 43 Batch 1100 Loss 0.0493\n",
      "Epoch 43 Batch 1200 Loss 0.0571\n",
      "Epoch 43 Batch 1300 Loss 0.0393\n",
      "Epoch 43 Batch 1400 Loss 0.0568\n",
      "Epoch 43 Batch 1500 Loss 0.0471\n",
      "Epoch 43 Batch 1600 Loss 0.0520\n",
      "Epoch 43 Batch 1700 Loss 0.0269\n",
      "Epoch 43 Batch 1800 Loss 0.0566\n",
      "Epoch 43 Batch 1900 Loss 0.0389\n",
      "Epoch 43 Batch 2000 Loss 0.0482\n",
      "Epoch 43 Batch 2100 Loss 0.0631\n",
      "Epoch 43 Batch 2200 Loss 0.0377\n",
      "Epoch 43 Batch 2300 Loss 0.0723\n",
      "Epoch 43 Batch 2400 Loss 0.0294\n",
      "Epoch 43 Loss 0.0460\n",
      "Time taken for 1 epoch 157.37578082084656 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0481\n",
      "Epoch 44 Batch 100 Loss 0.0197\n",
      "Epoch 44 Batch 200 Loss 0.0459\n",
      "Epoch 44 Batch 300 Loss 0.0490\n",
      "Epoch 44 Batch 400 Loss 0.0498\n",
      "Epoch 44 Batch 500 Loss 0.0304\n",
      "Epoch 44 Batch 600 Loss 0.0402\n",
      "Epoch 44 Batch 700 Loss 0.0478\n",
      "Epoch 44 Batch 800 Loss 0.0315\n",
      "Epoch 44 Batch 900 Loss 0.0466\n",
      "Epoch 44 Batch 1000 Loss 0.0447\n",
      "Epoch 44 Batch 1100 Loss 0.0445\n",
      "Epoch 44 Batch 1200 Loss 0.0526\n",
      "Epoch 44 Batch 1300 Loss 0.0666\n",
      "Epoch 44 Batch 1400 Loss 0.0667\n",
      "Epoch 44 Batch 1500 Loss 0.0392\n",
      "Epoch 44 Batch 1600 Loss 0.0500\n",
      "Epoch 44 Batch 1700 Loss 0.0392\n",
      "Epoch 44 Batch 1800 Loss 0.0510\n",
      "Epoch 44 Batch 1900 Loss 0.0600\n",
      "Epoch 44 Batch 2000 Loss 0.0386\n",
      "Epoch 44 Batch 2100 Loss 0.0470\n",
      "Epoch 44 Batch 2200 Loss 0.0502\n",
      "Epoch 44 Batch 2300 Loss 0.0514\n",
      "Epoch 44 Batch 2400 Loss 0.0535\n",
      "Epoch 44 Loss 0.0460\n",
      "Time taken for 1 epoch 157.39574027061462 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0411\n",
      "Epoch 45 Batch 100 Loss 0.0549\n",
      "Epoch 45 Batch 200 Loss 0.0350\n",
      "Epoch 45 Batch 300 Loss 0.0602\n",
      "Epoch 45 Batch 400 Loss 0.0288\n",
      "Epoch 45 Batch 500 Loss 0.0315\n",
      "Epoch 45 Batch 600 Loss 0.0269\n",
      "Epoch 45 Batch 700 Loss 0.0467\n",
      "Epoch 45 Batch 800 Loss 0.0250\n",
      "Epoch 45 Batch 900 Loss 0.0484\n",
      "Epoch 45 Batch 1000 Loss 0.0675\n",
      "Epoch 45 Batch 1100 Loss 0.0223\n",
      "Epoch 45 Batch 1200 Loss 0.0339\n",
      "Epoch 45 Batch 1300 Loss 0.0365\n",
      "Epoch 45 Batch 1400 Loss 0.0519\n",
      "Epoch 45 Batch 1500 Loss 0.0515\n",
      "Epoch 45 Batch 1600 Loss 0.0446\n",
      "Epoch 45 Batch 1700 Loss 0.0388\n",
      "Epoch 45 Batch 1800 Loss 0.0317\n",
      "Epoch 45 Batch 1900 Loss 0.0545\n",
      "Epoch 45 Batch 2000 Loss 0.0398\n",
      "Epoch 45 Batch 2100 Loss 0.0652\n",
      "Epoch 45 Batch 2200 Loss 0.0482\n",
      "Epoch 45 Batch 2300 Loss 0.0444\n",
      "Epoch 45 Batch 2400 Loss 0.0499\n",
      "Epoch 45 Loss 0.0456\n",
      "Time taken for 1 epoch 157.23373317718506 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0318\n",
      "Epoch 46 Batch 100 Loss 0.0524\n",
      "Epoch 46 Batch 200 Loss 0.0311\n",
      "Epoch 46 Batch 300 Loss 0.0387\n",
      "Epoch 46 Batch 400 Loss 0.0559\n",
      "Epoch 46 Batch 500 Loss 0.0246\n",
      "Epoch 46 Batch 600 Loss 0.0353\n",
      "Epoch 46 Batch 700 Loss 0.0660\n",
      "Epoch 46 Batch 800 Loss 0.0302\n",
      "Epoch 46 Batch 900 Loss 0.0589\n",
      "Epoch 46 Batch 1000 Loss 0.0475\n",
      "Epoch 46 Batch 1100 Loss 0.0696\n",
      "Epoch 46 Batch 1200 Loss 0.0291\n",
      "Epoch 46 Batch 1300 Loss 0.0333\n",
      "Epoch 46 Batch 1400 Loss 0.0623\n",
      "Epoch 46 Batch 1500 Loss 0.0392\n",
      "Epoch 46 Batch 1600 Loss 0.0513\n",
      "Epoch 46 Batch 1700 Loss 0.0434\n",
      "Epoch 46 Batch 1800 Loss 0.0357\n",
      "Epoch 46 Batch 1900 Loss 0.0274\n",
      "Epoch 46 Batch 2000 Loss 0.0343\n",
      "Epoch 46 Batch 2100 Loss 0.0760\n",
      "Epoch 46 Batch 2200 Loss 0.0335\n",
      "Epoch 46 Batch 2300 Loss 0.0588\n",
      "Epoch 46 Batch 2400 Loss 0.0457\n",
      "Epoch 46 Loss 0.0458\n",
      "Time taken for 1 epoch 157.1616244316101 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0385\n",
      "Epoch 47 Batch 100 Loss 0.0350\n",
      "Epoch 47 Batch 200 Loss 0.0184\n",
      "Epoch 47 Batch 300 Loss 0.0585\n",
      "Epoch 47 Batch 400 Loss 0.0462\n",
      "Epoch 47 Batch 500 Loss 0.0315\n",
      "Epoch 47 Batch 600 Loss 0.0302\n",
      "Epoch 47 Batch 700 Loss 0.0147\n",
      "Epoch 47 Batch 800 Loss 0.0437\n",
      "Epoch 47 Batch 900 Loss 0.0589\n",
      "Epoch 47 Batch 1000 Loss 0.0326\n",
      "Epoch 47 Batch 1100 Loss 0.0606\n",
      "Epoch 47 Batch 1200 Loss 0.0471\n",
      "Epoch 47 Batch 1300 Loss 0.0272\n",
      "Epoch 47 Batch 1400 Loss 0.0515\n",
      "Epoch 47 Batch 1500 Loss 0.0603\n",
      "Epoch 47 Batch 1600 Loss 0.0611\n",
      "Epoch 47 Batch 1700 Loss 0.0449\n",
      "Epoch 47 Batch 1800 Loss 0.0865\n",
      "Epoch 47 Batch 1900 Loss 0.0598\n",
      "Epoch 47 Batch 2000 Loss 0.0434\n",
      "Epoch 47 Batch 2100 Loss 0.0394\n",
      "Epoch 47 Batch 2200 Loss 0.0481\n",
      "Epoch 47 Batch 2300 Loss 0.0433\n",
      "Epoch 47 Batch 2400 Loss 0.0572\n",
      "Epoch 47 Loss 0.0454\n",
      "Time taken for 1 epoch 157.17911386489868 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0331\n",
      "Epoch 48 Batch 100 Loss 0.0240\n",
      "Epoch 48 Batch 200 Loss 0.0451\n",
      "Epoch 48 Batch 300 Loss 0.0518\n",
      "Epoch 48 Batch 400 Loss 0.0420\n",
      "Epoch 48 Batch 500 Loss 0.0545\n",
      "Epoch 48 Batch 600 Loss 0.0433\n",
      "Epoch 48 Batch 700 Loss 0.0346\n",
      "Epoch 48 Batch 800 Loss 0.0353\n",
      "Epoch 48 Batch 900 Loss 0.0335\n",
      "Epoch 48 Batch 1000 Loss 0.0405\n",
      "Epoch 48 Batch 1100 Loss 0.0349\n",
      "Epoch 48 Batch 1200 Loss 0.0495\n",
      "Epoch 48 Batch 1300 Loss 0.0407\n",
      "Epoch 48 Batch 1400 Loss 0.0363\n",
      "Epoch 48 Batch 1500 Loss 0.0338\n",
      "Epoch 48 Batch 1600 Loss 0.0517\n",
      "Epoch 48 Batch 1700 Loss 0.0617\n",
      "Epoch 48 Batch 1800 Loss 0.0465\n",
      "Epoch 48 Batch 1900 Loss 0.0422\n",
      "Epoch 48 Batch 2000 Loss 0.0423\n",
      "Epoch 48 Batch 2100 Loss 0.0552\n",
      "Epoch 48 Batch 2200 Loss 0.0551\n",
      "Epoch 48 Batch 2300 Loss 0.0328\n",
      "Epoch 48 Batch 2400 Loss 0.0336\n",
      "Epoch 48 Loss 0.0448\n",
      "Time taken for 1 epoch 157.26106429100037 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0283\n",
      "Epoch 49 Batch 100 Loss 0.0146\n",
      "Epoch 49 Batch 200 Loss 0.0321\n",
      "Epoch 49 Batch 300 Loss 0.0548\n",
      "Epoch 49 Batch 400 Loss 0.0420\n",
      "Epoch 49 Batch 500 Loss 0.0387\n",
      "Epoch 49 Batch 600 Loss 0.0413\n",
      "Epoch 49 Batch 700 Loss 0.0366\n",
      "Epoch 49 Batch 800 Loss 0.0265\n",
      "Epoch 49 Batch 900 Loss 0.0477\n",
      "Epoch 49 Batch 1000 Loss 0.0439\n",
      "Epoch 49 Batch 1100 Loss 0.0488\n",
      "Epoch 49 Batch 1200 Loss 0.0424\n",
      "Epoch 49 Batch 1300 Loss 0.0334\n",
      "Epoch 49 Batch 1400 Loss 0.0507\n",
      "Epoch 49 Batch 1500 Loss 0.0582\n",
      "Epoch 49 Batch 1600 Loss 0.0372\n",
      "Epoch 49 Batch 1700 Loss 0.0361\n",
      "Epoch 49 Batch 1800 Loss 0.0372\n",
      "Epoch 49 Batch 1900 Loss 0.0444\n",
      "Epoch 49 Batch 2000 Loss 0.0451\n",
      "Epoch 49 Batch 2100 Loss 0.0440\n",
      "Epoch 49 Batch 2200 Loss 0.0669\n",
      "Epoch 49 Batch 2300 Loss 0.0333\n",
      "Epoch 49 Batch 2400 Loss 0.0932\n",
      "Epoch 49 Loss 0.0447\n",
      "Time taken for 1 epoch 157.28541684150696 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0323\n",
      "Epoch 50 Batch 100 Loss 0.0315\n",
      "Epoch 50 Batch 200 Loss 0.0220\n",
      "Epoch 50 Batch 300 Loss 0.0459\n",
      "Epoch 50 Batch 400 Loss 0.0354\n",
      "Epoch 50 Batch 500 Loss 0.0172\n",
      "Epoch 50 Batch 600 Loss 0.0608\n",
      "Epoch 50 Batch 700 Loss 0.0776\n",
      "Epoch 50 Batch 800 Loss 0.0408\n",
      "Epoch 50 Batch 900 Loss 0.0442\n",
      "Epoch 50 Batch 1000 Loss 0.0403\n",
      "Epoch 50 Batch 1100 Loss 0.0378\n",
      "Epoch 50 Batch 1200 Loss 0.0410\n",
      "Epoch 50 Batch 1300 Loss 0.0473\n",
      "Epoch 50 Batch 1400 Loss 0.0675\n",
      "Epoch 50 Batch 1500 Loss 0.0615\n",
      "Epoch 50 Batch 1600 Loss 0.0592\n",
      "Epoch 50 Batch 1700 Loss 0.0511\n",
      "Epoch 50 Batch 1800 Loss 0.0636\n",
      "Epoch 50 Batch 1900 Loss 0.0463\n",
      "Epoch 50 Batch 2000 Loss 0.0530\n",
      "Epoch 50 Batch 2100 Loss 0.0310\n",
      "Epoch 50 Batch 2200 Loss 0.0628\n",
      "Epoch 50 Batch 2300 Loss 0.0451\n",
      "Epoch 50 Batch 2400 Loss 0.0652\n",
      "Epoch 50 Loss 0.0451\n",
      "Time taken for 1 epoch 157.6116819381714 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  #инициализируем входное скрытое состояние (из нулей) размера (батч, кол-во рекуррентных ячеек)\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    #делаем шаг обучения и считаем ошибку\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  #сохраняем checkpoint каждые 25 эпох\n",
    "  if (epoch + 1) % 25 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "EbQpyYs13jF_"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  # строим матрицу внимания из нулей размера (макс длина таргета, макс длина входа)  \n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  # выполняем препроцессинг предложений\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # разбиваем предложение по пробелам и составляем список индексов каждого слова\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  # дополняем inputs нулями справа до максимальной длины входного текста\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  # преобразуем inputs в тензор\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  # инициализируем входной hidden из нулей размера (1, units)\n",
    "  hidden = [tf.zeros((1, units))]\n",
    "  # подаем inputs и hidden в encoder\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  # инициализируем входной hidden декодера -- выходной хидден энкодера\n",
    "  dec_hidden = enc_hidden\n",
    "  # вход декодера -- список [индекс start] размера(1,1)\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    # получаем выход декодера\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # сохраняем веса внимания, чтобы позже визуализировать\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    # заканчиваем на токене end\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # предсказанный predicted ID подаем обратно в декодер (размер (1,1))\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Qjy26Fe8Bv5d"
   },
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  \"\"\"функция визуализации\"\"\"\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "sl9zUHzg3jGI"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n250XbnjOaqP"
   },
   "source": [
    "## Переводы и визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UJpT9D5_OgP6",
    "outputId": "84b90562-76d5-414b-9dfc-0c956b91812c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f0806472020>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "id": "WrAM0FDomq3E",
    "outputId": "e709a43b-bf36-4bea-c0e8-95de8f477e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> отличная погода ! <end>\n",
      "Predicted translation: great weather ! <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101/340526640.py:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/tmp/ipykernel_101/340526640.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('Отличная погода!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "5bhFfwcIMX5i",
    "outputId": "b18f32f0-4eee-4911-afec-7925171ed3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> что ещ можно сказать ? <end>\n",
      "Predicted translation: what more can be said ? <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_101/340526640.py:9: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
      "/tmp/ipykernel_101/340526640.py:10: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('Что ещё можно сказать?')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
